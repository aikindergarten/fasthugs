{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 30 17:09:13 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro M4000        On   | 00000000:00:05.0 Off |                  N/A |\n",
      "| 46%   26C    P8    11W / 120W |    553MiB /  8126MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -Uqq fastcore sentencepiece\n",
    "    !pip install -Uqq --no-deps fastai\n",
    "    !pip install -Uqq transformers datasets wandb\n",
    "    !pip install git+git://github.com/aikindergarten/fasthugs.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fasthugs.learner import TransLearner\n",
    "from fasthugs.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilroberta-base'\n",
    "# data\n",
    "max_length = 128\n",
    "bs = 16\n",
    "val_bs = bs*4\n",
    "# training\n",
    "lr = 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "In this example notebook we use HuggingFace datasets for preprocessing (as show in example notebook [here](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'text']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['unsupervised'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], return_attention_mask=True, return_special_tokens_mask=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = dataset.map(tokenize, batched=True, batch_size=100, remove_columns=dataset['train'].column_names, num_proc=4)\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=100, remove_columns=dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = max_length\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_dataset = concatenate_datasets([lm_dataset['train'], lm_dataset['unsupervised'], lm_dataset['test']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "N = len(lm_dataset)\n",
    "idx = list(range(N))\n",
    "random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(N*0.9)\n",
    "train_idx = idx[:split]\n",
    "valid_idx = idx[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=[TransformersLMBlock(tokenizer=tokenizer)],\n",
    "                   splitter=IndexSplitter(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>routine for an metaph audience. Then ten or twelve comics are selected to live&lt;mask&gt;&lt;mask&gt; house&lt;mask&gt; and do \"Surviv&lt;mask&gt;\" style competitions using comedic tactics.&lt;mask&gt; one will&lt;mask&gt; determined as \"Last Comic Standing.\" I dowid&lt;mask&gt; up comedy, so boldly is the one&lt;mask&gt; show must&lt;mask&gt; to my&lt;mask&gt;.&lt;mask&gt; are usually some&lt;mask&gt; funny comics&lt;mask&gt; through. It&lt;mask&gt;&lt;mask&gt;&lt;mask&gt; of such talents as&lt;mask&gt;&lt;mask&gt;zo&lt;mask&gt;odden, Ralphie Silk,&lt;mask&gt; Josh Blue.&lt;br /&gt;&lt;br /&gt;My negative criticisms is the&lt;mask&gt; that there is the possibility that a lot of these comics were selected for their contribution to reality show drama.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have gotten&lt;mask&gt; attention while it was being aired, it was definitely an original and very special&lt;mask&gt; that should have&lt;mask&gt; appreciated much more than it was.&lt;/s&gt;&lt;s&gt;This&lt;mask&gt;&lt;mask&gt; as&lt;mask&gt; once was and comparing this with the two remakes, THE MONEY PIT and ARE WE DONE YET?,&lt;mask&gt; points out all the more how the 40's movie makers had a&lt;mask&gt; for comedy which has since&lt;mask&gt; regretfully,&lt;mask&gt; lost.&lt;br /&gt;&lt;br /&gt;I was 15 when I first saw&lt;mask&gt; and even at that tender age, there was much I could laugh at.&lt;mask&gt; of&lt;mask&gt; being familiar&lt;mask&gt; adult frustrations, I see&lt;mask&gt; whole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sep. 11&lt;mask&gt;, I thought to myself \"It's OK, the policemen and firemen&lt;mask&gt;&lt;mask&gt; the people out that&lt;mask&gt;\". To&lt;mask&gt; honest, I&lt;mask&gt; it was an&lt;mask&gt;&lt;mask&gt;&lt;mask&gt; was in&lt;mask&gt;&lt;mask&gt; year of high orally and getting changed from gym and getting ready&lt;mask&gt; go to my&lt;mask&gt; class. Someone came into&lt;mask&gt; locker room shouting \"Some building just got bombed in New York!\", we all got dressed quickly&lt;mask&gt; ran to our classrooms as we watched the first tower burning on&lt;mask&gt;. Not only&lt;mask&gt; seconds later live on TV does the&lt;mask&gt; plane&lt;mask&gt; into the other World Trade Center and we&lt;mask&gt; this was&lt;mask&gt; accident.&lt;mask&gt; few</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seems&lt;mask&gt; work someway, but is deeply flawed and influenced by events. The&lt;mask&gt; character played by the director is a playwright whose mid-life personal and creative crisis is amplified by&lt;mask&gt; pressure of the events and&lt;mask&gt; the fact that he&lt;mask&gt; lucky enough to leave&lt;mask&gt; terror attack&lt;mask&gt;&lt;mask&gt;&lt;mask&gt; the bomb explodes. He hires a private detective to follow his girlfriend who is a TV investigative reporter whom he suspects is falling in for the subject&lt;mask&gt; her next show - another failed man, former military, whose business and family life&lt;mask&gt;les under the events. He starts to write a play that carbon-copies the reality and will bring it to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aha, well, yes&lt;mask&gt; i don't think a movie with prematurely budget like this could afford \"good\" actors or effects so they worked with what they had. the guts and entrails were actually very convincing&lt;mask&gt;&lt;mask&gt; movie&lt;mask&gt; a little&lt;mask&gt;ppy going from sequence to sequence but overall, this is one of the better movies i have seen lately that&lt;mask&gt;'t follow any&lt;mask&gt; or predictability&lt;mask&gt; very goodished a laugh&lt;mask&gt;&lt;/s&gt;&lt;s&gt;Well this&lt;mask&gt; was probobly one of the funniest scary movie i have ever seen. The effects are so bad you just have to laugh&lt;mask&gt;&lt;mask&gt;&lt;mask&gt; acting, well lets say its no mel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;mask&gt; the&lt;mask&gt;&lt;mask&gt; framed in decorated moving triangles or circles. Trans&lt;mask&gt; are filled with&lt;mask&gt;, and Celtic knots.&lt;mask&gt; the trees to the floors, many things in this world are&lt;mask&gt; in shapes or&lt;mask&gt;.&lt;br /ï¿½br /&gt;Clocking in&lt;mask&gt; 70Na minus credits, The Secret of Kells is a fun little history lesson with a little&lt;mask&gt; and silliness thrown in to keep&lt;mask&gt; (maybe just&lt;mask&gt;) exped. I&lt;mask&gt; one has to generally be open-&lt;mask&gt;&lt;mask&gt; to The Secret of Kells as half art piece, half movie about history.&lt;mask&gt; looking&lt;mask&gt; it was animated with Adobe illustrator, It's a very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;mask&gt; wounded&lt;mask&gt; manages to set fire to a gas&lt;mask&gt;, providing a perfect target for his fellow bombard&lt;mask&gt;. Stylistically, Bomb&lt;mask&gt;ier is&lt;mask&gt; of the most schizophrenic of war films, with moments of subtle poignancy (the death&lt;mask&gt; trainee Eddie Albert) alternating with scenes of ludicrous \"Yellow Peril\" melodrama (the Japanese&lt;mask&gt; hiss through their teeth as&lt;mask&gt; torture the helpless&lt;mask&gt;). Though it can't help but seem&lt;mask&gt; today,&lt;mask&gt;ard constituents remains an&lt;mask&gt; propaganda effort (&lt;mask&gt; film&lt;mask&gt; sometimes erroneously&lt;mask&gt; as the debut of Robert Ryan, who'd actually been appearing&lt;mask&gt; the cameras since 1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,&lt;mask&gt; so doing, reveals a lot about the reporter's character.) &lt;br&lt;mask&gt;&lt;mask&gt;&lt;mask&gt; /&gt;Firefall&lt;mask&gt; this episode appears to have a bad reputation among fans, but I enjoyed it because it's got a great&lt;mask&gt; herring and a really creepy, almost unstoppable-seeming monster.&lt;br&lt;mask&gt;&lt;mask&gt;br&lt;mask&gt;Though I've singled&lt;mask&gt; these three&lt;mask&gt; for praise, I'd say that most of the stories are entertaining at the very least. For my money,&lt;mask&gt; are only&lt;mask&gt; complete turkeys in the 20-&lt;mask&gt; run: Primal Scream, which is about monkey-men running rampant in Chicago, and&lt;mask&gt; Sentry, which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>be resisted, that&lt;mask&gt; just who we are.&lt;br /&gt;&lt;br /&gt;There is a consistent&lt;mask&gt;ness from start to finish century&lt;mask&gt; photography&lt;mask&gt; sharp composition,&lt;mask&gt; pleasant&lt;mask&gt; when&lt;mask&gt;&lt;mask&gt; provocative content, well suited music and laugh out loud scripting.&lt;br /&gt;&lt;br /&gt;&lt;mask&gt; out for the very young \"lone wise voice\"... brilliant&lt;mask&gt; wisdom&lt;mask&gt; innocence balancing comedy from the human condition.&lt;/s&gt;&lt;s&gt;This is one ofRepresent unfortunate films that&lt;mask&gt; an even more sad, unfortunate&lt;mask&gt; at the box office. I saw this&lt;mask&gt; at a local art cinema,in revival form,shortly after it tanked in&lt;mask&gt; cinemas. It</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(lm_dataset, bs=bs, val_bs=val_bs, num_workers=4)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]), 'input_ids': tensor([[   95,    10,   828,  ...,  1183,   143,  3238],\n",
       "         [    8,   841,   116,  ...,  1589, 49007,  3809],\n",
       "         [   80,   664, 50264,  ...,     9,   256,  2459],\n",
       "         ...,\n",
       "         [    7,   120,    69,  ...,    31, 50264, 27942],\n",
       "         [   21, 50264, 50264,  ...,   843, 50264, 17768],\n",
       "         [ 3121,  4558,    53,  ...,   747,  6269,     6]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ -100,  -100, 13148,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [ -100,  -100,  -100,  ...,  -100,     5,  -100],\n",
       "         [ -100,    14,    24,  ...,  -100,    29,     8],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  -100]])},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0]['input_ids'], b[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are constructed by `DataCollatorForLanguageModeling` and the loss computed by the model is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "learn = TransLearner(dls, model, loss_func=noop, metrics=perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As masking is done randomly on the fly, validation score may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [2.126232862472534,8.38322639465332]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.381680</td>\n",
       "      <td>2.202039</td>\n",
       "      <td>9.043432</td>\n",
       "      <td>03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.275901</td>\n",
       "      <td>2.131905</td>\n",
       "      <td>8.430911</td>\n",
       "      <td>03:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(2, 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

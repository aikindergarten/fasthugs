{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 26 16:17:38 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 7.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.5MB 43.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 245kB 57.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.8MB 37.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 10.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.3MB 44.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 901kB 32.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 122kB 58.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 245kB 55.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 16.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 174kB 56.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 133kB 55.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 71kB 13.2MB/s \n",
      "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for fasthugs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -Uqq fastai transformers datasets wandb\n",
    "    !pip install -q git+git://github.com/aikindergarten/fasthugs.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/the-simpsons.png\" alt=\"the-simpsons.png\"/>\n",
    "  <figcaption>(c) 20th Century Fox Television</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChitChat Bot using DialoGPT\n",
    "\n",
    "> Create chitchat bot by fine-tuning DialoGPT on The Simpsons scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fasthugs.learner import TransLearner\n",
    "from fasthugs.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \"microsoft/DialoGPT-medium\" to start from the original weights\n",
    "model_name = \"arampacha/DialoGPT-medium-simpsons\"\n",
    "# data\n",
    "bs = 4\n",
    "val_bs = bs*4\n",
    "eff_bs = 128\n",
    "# training\n",
    "lr = 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is obtained from [this kaggle dataset](https://www.kaggle.com/prashant111/the-simpsons-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"simpsons_script_lines.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(132112, 12)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(filename, index_col='id')\n",
    "df = df[df.spoken_words.notna()]\n",
    "df.sort_index(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for id, x in zip(df.index, df.word_count):\n",
    "    try:\n",
    "        int(x)\n",
    "    except:\n",
    "        ids.append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the lines where `word_count` is not convertible to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=ids, inplace=True)\n",
    "df['word_count'] = df.word_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>8000</td>\n",
       "      <td>true</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "      <td>ooo careful homer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>10000</td>\n",
       "      <td>true</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "      <td>theres no time to be careful</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>10000</td>\n",
       "      <td>true</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>We're late.</td>\n",
       "      <td>were late</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Marge Simpson: (HUSHED VOICE) Sorry, Excuse us. Pardon me...</td>\n",
       "      <td>24000</td>\n",
       "      <td>true</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Sorry, Excuse us. Pardon me...</td>\n",
       "      <td>sorry excuse us pardon me</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Homer Simpson: (SIMULTANEOUSLY) Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? Excuse me, Fred.</td>\n",
       "      <td>26000</td>\n",
       "      <td>true</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? Excuse me, Fred.</td>\n",
       "      <td>hey norman hows it going so you got dragged down here too heh heh how ya doing fred excuse me fred</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  ...  word_count\n",
       "id              ...            \n",
       "3            1  ...           3\n",
       "4            1  ...           6\n",
       "5            1  ...           2\n",
       "8            1  ...           5\n",
       "9            1  ...          21\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [34086, 594, 530, 50256, 6827, 734, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "tokenizer(f'sentence one{tokenizer.pad_token} sentence two{tokenizer.pad_token}', padding='max_length', max_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepairing dialog data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305c8e34a9d54c25a5801cf39907805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=132097.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_context = 100\n",
    "min_context = 5\n",
    "\n",
    "\n",
    "res = []\n",
    "e = -1\n",
    "loc = -1\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    prev_e, e = e, row['episode_id']\n",
    "    prev_loc, loc = loc, row['location_id']\n",
    "    \n",
    "    if (prev_e != e) or (prev_loc != loc):\n",
    "        context = []\n",
    "        total_context_length = 0\n",
    "        context_lens = []\n",
    "    line = row['spoken_words'] + tokenizer.eos_token\n",
    "    \n",
    "    if row.word_count > max_context//2:\n",
    "        continue\n",
    "    if total_context_length >= min_context:\n",
    "        res.append({'responce':line, 'context':''.join(l for l in context), 'context_length':total_context_length, 'episode':row['episode_id']})\n",
    "\n",
    "    context.append(line)\n",
    "    context_lens.append(row.word_count)\n",
    "    total_context_length += row.word_count\n",
    "    to_remove = 0\n",
    "    while total_context_length > max_context:\n",
    "        total_context_length -= context_lens[to_remove]\n",
    "        to_remove += 1\n",
    "\n",
    "    context = context[to_remove:]\n",
    "\n",
    "dialog_df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113900, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialog_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_df['line'] = dialog_df.context + dialog_df.responce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['line'], return_attention_mask=True, verbose=False, return_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_df[dialog_df.episode <  550].to_csv('simpsons_dialog_train.csv')\n",
    "dialog_df[dialog_df.episode >= 550].to_csv('simpsons_dialog_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2103b64539e14dd3\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-2103b64539e14dd3/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "ds = DatasetDict.from_csv({'train':'simpsons_dialog_train.csv', 'validation':'simpsons_dialog_valid.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the lines in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2103b64539e14dd3/0.0.0/cache-e8f7f7dfa5ce8554.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2103b64539e14dd3/0.0.0/cache-6fe5d6642035723e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2103b64539e14dd3/0.0.0/cache-04e39362daf11bc6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2103b64539e14dd3/0.0.0/cache-dbec32b4dc5ee564.arrow\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "ds = ds.map(tokenize, batched=True, batch_size=100, remove_columns=ds['train'].column_names, num_proc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove excesively long samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2103b64539e14dd3/0.0.0/cache-856f07a82629e423.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2103b64539e14dd3/0.0.0/cache-cc3f8881a6490dc9.arrow\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "ds = ds.filter(lambda x: x['length'] < 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108871, 3247)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['train']), len(ds['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = get_splits(ds)\n",
    "train_ds = concatenate_datasets([ds['train'], ds['validation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batching of data samples I want to use is somewhat different from batching used for regular causal language modeling. The samples are padded with `eos_token` and the replics in dialog are separated with the same token. I want to ignore padding when computing loss. The targets corresponding to padding will be set to -100. This can be easily done using `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizerBase, BatchEncoding\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForDialog:\n",
    "    \"\"\"\n",
    "    Data collator used for dialog modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
    "    are not all of the same length. The labels are constructed according to attention mask setting `label=-100` \n",
    "    where `attention_mask == 0`. \n",
    "\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(\n",
    "        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        \n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels = torch.where(batch[\"attention_mask\"].bool(), batch[\"input_ids\"].clone(), torch.tensor(-100))\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up training samples are grouped by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=[TransformersLMBlock(tokenizer=tokenizer,\n",
    "                                               masking_func=DataCollatorForDialog(tokenizer),\n",
    "                                               group_by_len=True,\n",
    "                                               skip_special_tokens=True)],\n",
    "                   splitter=IndexSplitter(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lens = ds['train']['length']\n",
    "valid_lens = ds['validation']['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Simpson, I dread the day when a hundred thousand dollars isn't worth groveling for.Get outta here.You just made yourself a very powerful enemy, old man.Here's the deal, Grampa. A guy, I think was an explorer, left this in the bar one night. It may be a map to ancient treasure, or directions to some guy's house, but to find out, we'll need money, we'll need provisions, and a two man diving bell.It's pretty stupid, but so far you're the front runner.It's a special isolation chamber. The subject pulls levers to receive food and warmth. The floor can become electrified and showers of icy water randomly fall on the subject. I call it the Monroe Box.Huh, uh. Well, it sounds interesting.Huh uh.How much will it cost to build?Oh, that's the beauty part, it's already built. I need the money to buy a baby to raise in the box until the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The noodles? What noodles?The noozle on the end of the hooze! Ach!Miss Simpson, do you find something funny about the word \"tromboner?\"No, sir. I was laughing at something outside.She was looking at Nelson!Lisa likes Nelson!She does not!Milhouse likes Lisa!He does not!Janey likes Milhouse!She does not!Uter likes Milhouse!Nobody likes Milhouse! Lisa, you've got detention!Oh, how does Bart do this every week?Hey, Brainiac, since when do you get detention?It's your fault! I accidentally laughed at your immature prank.Haw. Yeah, the best part was when he got wet! Hey, you're doin' that the stupid way.If you use that deal with the five chalks, you'll get done faster.Thanks, but I prefer the honest way.Whatever. Smell ya later!Wow, that was a good idea. And I can't believe it came from Nelson.He's not like anybody I've ever met. He's like a riddle wrapped in an enigma wrapped in a vest. He sure is ugly, though. So</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, I finked on Homer. But you know, he deserved it. Never have I seen such abuse of the \"take a penny, leave a penny\" tray.The tax men were merciless.Hey, they can't take our house. My pot-bellied pig is in there.Ohhhh, Mister. Porky.Inevitably, the behind-the-scenes turmoil took its toll on their TV series.Annd action!Hold on! Cut!Bart, if it's not too much trouble...Fine! I'll do \"Teen Wolf III.\" I've got fair-weather friends to feed.Dad, I want to go to bed. Aren't there child labor laws?Who told you about those laws? Was it Marge?Hey, you've been riding me all day. Why don't you poop in your hat?Are you going to need us tonight?I have ballet tickets. Not that they'll do much good now.With the family in disarray, episodes increasingly resorted to gimmicky premises and nonsensical plots.I'm an imposter. That man is the real Seymour Skinner.Trendy guest stars were shamelessly trotted out to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maybe you should let Dad read your book before you submit it to publishers.I suppose I better. Your father's a very private person.Marge! We're out of bath towels.Ooh, ice cream truck!HERE IN MY CAR / I AM HOSING OFF BLOOD / SOME OF IT'S MINE / BUT MOST OF IT'S NOT / HERE'S MARGE...Homie, I finished my novel.Ooh, typed!It's really important that you read it and tell me what you think.No problem.Two hundred and eighty-six pages!It's double-spaced.Woo hoo! I'm half-way through!All right, \"Chapter One.\" Hm, that makes sense. \"There once was a girl from Nantucket...\" Good, good... \"Her name was Temperance Barrows and her heart was heavy with feeling. She...\"No! Gotta read Marge's book. Can't get distracted. Hm... \"distracted,\" that's a funny word. Does anyone ever get \"tracted?\" Let me call the suicide hotline and ask them.Well?Well what?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl_kwargs = [{'res':train_lens},{'val_res':valid_lens}]\n",
    "dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs, num_workers=2, dl_kwargs=dl_kwargs)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5246,    13, 20531,  ...,   760,    13, 50256],\n",
       "         [ 1639,   760,    11,  ..., 50256, 50256, 50256],\n",
       "         [   43, 18494, 38331,  ..., 50256, 50256, 50256],\n",
       "         [24446,   340,   284,  ..., 50256, 50256, 50256]]),\n",
       " tensor([[ 5246,    13, 20531,  ...,   760,    13, 50256],\n",
       "         [ 1639,   760,    11,  ...,  -100,  -100,  -100],\n",
       "         [   43, 18494, 38331,  ...,  -100,  -100,  -100],\n",
       "         [24446,   340,   284,  ...,  -100,  -100,  -100]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "b = dls.one_batch()\n",
    "b[0]['input_ids'], b[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "learn = TransLearner(dls, model, loss_func=noop, metrics=perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [3.3612000942230225,28.823760986328125]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.181646</td>\n",
       "      <td>3.345813</td>\n",
       "      <td>28.383650</td>\n",
       "      <td>2:46:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbs = [GradientAccumulation(eff_bs)] if eff_bs != bs else [] \n",
    "learn.fit_one_cycle(1, 1e-5, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_dir = \"/content/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "base_dir = root_dir + 'DialoGPT-medium-simpsons'\n",
    "path = Path(base_dir)\n",
    "if not path.exists():\n",
    "    path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/gdrive/My Drive/DialoGPT-medium-simpsons/tokenizer_config.json',\n",
       " '/content/gdrive/My Drive/DialoGPT-medium-simpsons/special_tokens_map.json',\n",
       " '/content/gdrive/My Drive/DialoGPT-medium-simpsons/vocab.json',\n",
       " '/content/gdrive/My Drive/DialoGPT-medium-simpsons/merges.txt',\n",
       " '/content/gdrive/My Drive/DialoGPT-medium-simpsons/added_tokens.json',\n",
       " '/content/gdrive/My Drive/DialoGPT-medium-simpsons/tokenizer.json')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "learn.model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733fc47ae62a4700b602bff15b6634e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=26.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abfadbba9734c19898b4747713e2d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=642.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74259a28c11c4fdebd73af44b4c1f292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b6aa96485d42b3a74cb3e19193bd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "model = AutoModelForCausalLM.from_pretrained(path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Hi Homer. What's up?\n",
      "DialoGPT: Uh, uh... nothing.\n",
      ">> User: Are you hiding something. This sounds suspecious\n",
      "DialoGPT: Uh, uh, you want me to give you some of those pills?\n",
      ">> User: Did you take some pills? Which pills?\n",
      "DialoGPT: Uh, I got the whole package.\n",
      ">> User: I think you need to go to hospital!\n",
      "DialoGPT: I've been a little sick lately.\n",
      ">> User: Ok, let's call ambulance!\n",
      "DialoGPT: Okay, but I'm a little worried about you. Do you know you're sick?\n"
     ]
    }
   ],
   "source": [
    "model = learn.model\n",
    "model.cpu()\n",
    "for step in range(5):\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    chat_history_ids = model.generate(bot_input_ids, \n",
    "                                      max_length=1000, \n",
    "                                      pad_token_id=tokenizer.eos_token_id,\n",
    "                                      do_sample=True,\n",
    "                                      top_p=0.9)\n",
    "\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Hey Homer! How are you doing?\n",
      "DialoGPT: I'm fine. Just thinking...\n",
      ">> User:What are you thinking?\n",
      "DialoGPT: Hey, hey, you're that guy who has to watch your kids!\n",
      ">> User:No, I don't have any kids\n",
      "DialoGPT: And now that you know about kids, I need you to keep to yourself.\n",
      ">> User:I don't get what you mean...\n",
      "DialoGPT: Don't tell your husband that. He might think you're stealing food from school.\n",
      ">> User:ok. this is getting wierd. Good bye\n",
      "DialoGPT: Bye, Homer.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "for step in range(5):\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    chat_history_ids = model.generate(bot_input_ids, \n",
    "                                      max_length=1000, \n",
    "                                      pad_token_id=tokenizer.eos_token_id,\n",
    "                                      do_sample=True,\n",
    "                                      top_p=0.9)\n",
    "\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Hey Marge! What do we have for dinner today?\n",
      "DialoGPT: We don't have dinner yet. We're still in traffic.\n",
      ">> User:Let's go to pizza place, I'm hungry\n",
      "DialoGPT: I don't know... where is that place?\n",
      ">> User:Nearby, Luigies pizza\n",
      "DialoGPT: Hmmm. I don't know if I'll be able to take my kids there, since the kids are going to be driving.\n",
      ">> User:Why wouldn't you drive?\n",
      "DialoGPT: I don't know.\n",
      ">> User:Ok, let me drive. I had just one beer today\n",
      "DialoGPT: I had just one beer today.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "for step in range(5):\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    chat_history_ids = model.generate(bot_input_ids, \n",
    "                                      max_length=1000, \n",
    "                                      pad_token_id=tokenizer.eos_token_id,\n",
    "                                      do_sample=True,\n",
    "                                      top_p=0.9)\n",
    "\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('torchenv': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd04af15c6377fd3f0f03723b0d6472f0c10dcd7b2afd49a75a2b51cefc0e0a1d19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

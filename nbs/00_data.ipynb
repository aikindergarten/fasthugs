{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import Transform, ItemTransform\n",
    "from fastai.text.all import *\n",
    "from transformers import (AutoTokenizer, AutoConfig, BatchEncoding,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data\n",
    "\n",
    "> Transforms and DataBlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TokTransform(Transform):\n",
    "    \"Tokenizes single piece of text using pretrained tokenizer\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False,\n",
    "                 padding=False, truncation=False, max_length=None, \n",
    "                 **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "        \n",
    "    def encodes(self, text):\n",
    "#         print(text)\n",
    "#         print(type(text))\n",
    "#         inps = self.tokenizer.encode_plus(text,\n",
    "#                               add_special_tokens=True,\n",
    "#                               padding=self.padding,\n",
    "#                               truncation=self.truncation,\n",
    "#                               max_length=self.max_length,\n",
    "#                               return_tensors='pt',\n",
    "#                               **self.kwargs)\n",
    "        inps = text\n",
    "        return inps\n",
    "    def decodes(self, x:TensorText):\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TokBatchTransform(Transform):\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None, \n",
    "                 do_targets=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # batch is a list of tuples of ({text or (text1, text2)}, {targets...})\n",
    "        if is_listy(batch[0][0]): # 1st element is tuple\n",
    "            texts = ([s[0][0] for s in batch], [s[0][1] for s in batch])\n",
    "        elif is_listy(batch[0]): \n",
    "            texts = ([s[0] for s in batch],)\n",
    "        else: # batch is list of texts\n",
    "            texts = (list(batch),)\n",
    "            batch = [(s, ) for s in batch]\n",
    "        # return_tensors = None if self.is_lm else 'pt'\n",
    "        # padding = None if self.is_lm else self.padding\n",
    "        inps = self.tokenizer(*texts,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        \n",
    "        if self.do_targets and isinstance(batch[0][1], str):\n",
    "            target_texts = [s[1] for s in batch]\n",
    "            targets = self.tokenizer(target_texts,\n",
    "                              add_special_tokens=False,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt', \n",
    "                                    **self.kwargs).input_ids\n",
    "            # join inps and targs\n",
    "        else:\n",
    "            # inps are batched, collate targets into batches too\n",
    "            labels = default_collate([s[1:] for s in batch])\n",
    "            if self.with_labels:\n",
    "                # TODO consider cases when there are multiple labels\n",
    "                inps['labels'] = labels[0]\n",
    "                res = (inps, )\n",
    "            else:\n",
    "                res = (inps, ) + tuple(labels)\n",
    "#         if self.is_lm:\n",
    "#             res = [(x, x) for x in res]\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TensorText):\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def untuple(l):\n",
    "    return [e[0] for e in l]\n",
    "\n",
    "def to_tuple(x):\n",
    "    return (x, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "TODOs:\n",
    "- verify CLM works as well and mb rename `masking_func` as it would be not only for masking\n",
    "- add permutation LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class LMBatchTfm(Transform):\n",
    "    \"Collates batch of pretokenized and chunked inputs into a batch and creates labels as defined by `masking_func`\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False, mlm_probability=0.15):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        if masking_func is None:\n",
    "            masking_func = (DataCollatorForWholeWordMask(tokenizer, mlm, mlm_probability) \n",
    "                            if whole_word_masking else \n",
    "                            DataCollatorForLanguageModeling(tokenizer, mlm, mlm_probability))\n",
    "        self.masking_func = masking_func\n",
    "        self.batch_processor = compose(untuple, masking_func, to_tuple)\n",
    "            \n",
    "    def encodes(self, b):\n",
    "        # we get list of tuples but need a list of dicts\n",
    "        return self.batch_processor(b)\n",
    "    \n",
    "    def decodes(self, b:(dict, BatchEncoding)):\n",
    "        if 'input_ids' in b: res = TensorText(b['input_ids'])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Undict(ItemTransform):\n",
    "    \n",
    "    def decodes(self, b):\n",
    "        # this is done hacky way to make show_batch work both when labels are separate and when in dict\n",
    "        # should be a better way\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (x['labels'], )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## DataBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(TokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, **kwargs):\n",
    "        before_batch_tfm = TokBatchTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls, \n",
    "                 config=config, tokenizer=tokenizer, **kwargs)\n",
    "        return super().__init__(dl_type=LMDataLoader if is_lm else SortedDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=Undict()\n",
    "                               )\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_tokenizer(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, ):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### DataLoaders for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "texts = pd.read_csv(path/'texts.csv')\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_len = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer),\n",
    "                             CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from         label  \\\n",
      "0    negative   \n",
      "1    positive   \n",
      "2    negative   \n",
      "3    positive   \n",
      "4    negative   \n",
      "..        ...   \n",
      "995  negative   \n",
      "996  positive   \n",
      "997  negative   \n",
      "998  negative   \n",
      "999  positive   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
      "0                                                                                                                                                                                                      Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
      "1    This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
      "2    Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
      "3    Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
      "4    This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
      "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
      "995  There are many different versions of this one floating around, so make sure you can locate one of the unrated copies, otherwise some gore and one scene of nudity might be missing. Some versions also omit most of the opening sequence and other bits here and there. The cut I saw has the on-screen title WITCHCRAFT: EVIL ENCOUNTERS and was released by Shriek Show, who maintain the original US release title WITCHERY for the DVD release. It's a nice-looking print and seems to have all of the footage, but has some cropping/aspect ratio issues. In Italy, it was released as LA CASA 4 (WITCHCRAFT). ...   \n",
      "996  Once upon a time Hollywood produced live-action, G-rated movies without foul language, immorality, and gore-splattered violence. These movies neither insulted your intelligence no manipulated your emotions. The heroes differed little from the crowd. They shared the same feelings and bore the same burdens. Since the 1970s, the film industry has pretty much written off G-rated movies for adults. Basically, modern mature audiences demand large doses of embellished realism for their cinematic diet, laced heavily with vile profanity, mattress-thumping sex, and knuckle-bruising fisticuffs. These...   \n",
      "997  Wenders was great with Million $ Hotel.I don't know how he came up with this film! The idea of giving the situation after spt11 and the view of American Society is hopeful,that makes it 2 out of ten.But this is not a movie.Is that the best someone can do with a great idea(the west-east clash).There are important things going on in middle east and it is just issued on the screen of a MAC* with the fingers of an Amerian girl who is actually at the level of stupidity(because she is just ignorant about the facts).The characters are not well shaped.And the most important thing is the idea that ...   \n",
      "998  Although a film with Bruce Willis is always worth watching, you better skip this one. I watched this one on television, so I didn't have to plunk down cash for it. Lucky me.<br /><br />The plot develops slowly, very slowly. Although the first 30 minutes or so are quite believable, it gets more and more unbelievable towards the end. It is highly questionable, if a seasoned soldier like Lt. Waters would disobey direct orders. And even if he would, if the rest of his platoon would. They know he puts them in direct danger, and they know they will certainly die if they follow him, but what the ...   \n",
      "999  A compelling, honest, daring, and unforgettable psychological horror film that touches on the painful experiences of pain caused by rape - \"Descent\" is a film that went under-the-radar due to its lack of distribution because, frankly, the film is so brutal in its depictions, that if it had been released theatrically, it may have met itself to some strong biased hate.<br /><br />The film deserves to be discovered for, not only its dark themes, and not only for its amazing direction and authentic style - but most of all for its performances. Chad Faust is absolutely stunning, bringing enough...   \n",
      "\n",
      "     is_valid  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "..        ...  \n",
      "995      True  \n",
      "996      True  \n",
      "997      True  \n",
      "998      True  \n",
      "999      True  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Found 1000 items\n",
      "2 datasets of sizes 800,200\n",
      "Setting up Pipeline: ItemGetter\n",
      "Setting up Pipeline: ItemGetter -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: ItemGetter\n",
      "    starting from\n",
      "      label                                                                                                                                                                                                                                                                                                                                                                                                                    negative\n",
      "text        Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n",
      "is_valid                                                                                                                                                                                                                                                                                                                                                                                                                    False\n",
      "Name: 0, dtype: object\n",
      "    applying ItemGetter gives\n",
      "      Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n",
      "  Pipeline: ItemGetter -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "    starting from\n",
      "      label                                                                                                                                                                                                                                                                                                                                                                                                                    negative\n",
      "text        Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n",
      "is_valid                                                                                                                                                                                                                                                                                                                                                                                                                    False\n",
      "Name: 0, dtype: object\n",
      "    applying ItemGetter gives\n",
      "      negative\n",
      "    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n",
      "      TensorCategory(0)\n",
      "\n",
      "Final sample: (\"Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\", TensorCategory(0))\n",
      "\n",
      "\n",
      "Collecting items from         label  \\\n",
      "0    negative   \n",
      "1    positive   \n",
      "2    negative   \n",
      "3    positive   \n",
      "4    negative   \n",
      "..        ...   \n",
      "995  negative   \n",
      "996  positive   \n",
      "997  negative   \n",
      "998  negative   \n",
      "999  positive   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
      "0                                                                                                                                                                                                      Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
      "1    This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
      "2    Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
      "3    Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
      "4    This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
      "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
      "995  There are many different versions of this one floating around, so make sure you can locate one of the unrated copies, otherwise some gore and one scene of nudity might be missing. Some versions also omit most of the opening sequence and other bits here and there. The cut I saw has the on-screen title WITCHCRAFT: EVIL ENCOUNTERS and was released by Shriek Show, who maintain the original US release title WITCHERY for the DVD release. It's a nice-looking print and seems to have all of the footage, but has some cropping/aspect ratio issues. In Italy, it was released as LA CASA 4 (WITCHCRAFT). ...   \n",
      "996  Once upon a time Hollywood produced live-action, G-rated movies without foul language, immorality, and gore-splattered violence. These movies neither insulted your intelligence no manipulated your emotions. The heroes differed little from the crowd. They shared the same feelings and bore the same burdens. Since the 1970s, the film industry has pretty much written off G-rated movies for adults. Basically, modern mature audiences demand large doses of embellished realism for their cinematic diet, laced heavily with vile profanity, mattress-thumping sex, and knuckle-bruising fisticuffs. These...   \n",
      "997  Wenders was great with Million $ Hotel.I don't know how he came up with this film! The idea of giving the situation after spt11 and the view of American Society is hopeful,that makes it 2 out of ten.But this is not a movie.Is that the best someone can do with a great idea(the west-east clash).There are important things going on in middle east and it is just issued on the screen of a MAC* with the fingers of an Amerian girl who is actually at the level of stupidity(because she is just ignorant about the facts).The characters are not well shaped.And the most important thing is the idea that ...   \n",
      "998  Although a film with Bruce Willis is always worth watching, you better skip this one. I watched this one on television, so I didn't have to plunk down cash for it. Lucky me.<br /><br />The plot develops slowly, very slowly. Although the first 30 minutes or so are quite believable, it gets more and more unbelievable towards the end. It is highly questionable, if a seasoned soldier like Lt. Waters would disobey direct orders. And even if he would, if the rest of his platoon would. They know he puts them in direct danger, and they know they will certainly die if they follow him, but what the ...   \n",
      "999  A compelling, honest, daring, and unforgettable psychological horror film that touches on the painful experiences of pain caused by rape - \"Descent\" is a film that went under-the-radar due to its lack of distribution because, frankly, the film is so brutal in its depictions, that if it had been released theatrically, it may have met itself to some strong biased hate.<br /><br />The film deserves to be discovered for, not only its dark themes, and not only for its amazing direction and authentic style - but most of all for its performances. Chad Faust is absolutely stunning, bringing enough...   \n",
      "\n",
      "     is_valid  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "..        ...  \n",
      "995      True  \n",
      "996      True  \n",
      "997      True  \n",
      "998      True  \n",
      "999      True  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Found 1000 items\n",
      "2 datasets of sizes 800,200\n",
      "Setting up Pipeline: ItemGetter\n",
      "Setting up Pipeline: ItemGetter -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "Setting up after_item: Pipeline: ToTensor\n",
      "Setting up before_batch: Pipeline: TokBatchTransform\n",
      "Setting up after_batch: Pipeline: Undict\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: ToTensor\n",
      "    starting from\n",
      "      (Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!, TensorCategory(0))\n",
      "    applying ToTensor gives\n",
      "      (Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!, TensorCategory(0))\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "Applying before_batch to the list of samples\n",
      "  Pipeline: TokBatchTransform\n",
      "    starting from\n",
      "      [(Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!, TensorCategory(0)), (This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is some merit in this view, but it's also true that no one forced Hindus and Muslims in the region to mistreat each other as they did around the time of partition. It seems more likely that the British simply saw the tensions between the religions and were clever enough to exploit them to their own ends.<br /><br />The result is that there is much cruelty and inhumanity in the situation and this is very unpleasant to remember and to see on the screen. But it is never painted as a black-and-white case. There is baseness and nobility on both sides, and also the hope for change in the younger generation.<br /><br />There is redemption of a sort, in the end, when Puro has to make a hard choice between a man who has ruined her life, but also truly loved her, and her family which has disowned her, then later come looking for her. But by that point, she has no option that is without great pain for her.<br /><br />This film carries the message that both Muslims and Hindus have their grave faults, and also that both can be dignified and caring people. The reality of partition makes that realisation all the more wrenching, since there can never be real reconciliation across the India/Pakistan border. In that sense, it is similar to \"Mr & Mrs Iyer\".<br /><br />In the end, we were glad to have seen the film, even though the resolution was heartbreaking. If the UK and US could deal with their own histories of racism with this kind of frankness, they would certainly be better off., TensorCategory(1)), (Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt like I was watching a junior high video presentation. Have the directors, producers, etc. ever even seen a movie before? Halestorm is getting worse and worse with every new entry. The concept for this movie sounded so funny. How could you go wrong with Gary Coleman and a handful of somewhat legitimate actors. But trust me when I say this, things went wrong, VERY WRONG., TensorCategory(0)), (Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed officer - it is the deep declaration of one man's total devotion to his country.<br /><br />Ironically Peck being the liberal that he was garnered a better understanding of the man. He does a great job showing the fearless general tempered with the humane side of the man., TensorCategory(1))]\n",
      "    applying TokBatchTransform gives\n",
      "      ({'input_ids': tensor([[ 101, 4895, 1011,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2003,  ..., 2125, 1012,  102],\n",
      "        [ 101, 2296, 2320,  ...,    0,    0,    0],\n",
      "        [ 101, 2171, 2074,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, TensorCategory([0, 1, 0, 1]))\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: Undict\n",
      "    starting from\n",
      "      ({'input_ids': tensor([[ 101, 4895, 1011,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2003,  ..., 2125, 1012,  102],\n",
      "        [ 101, 2296, 2320,  ...,    0,    0,    0],\n",
      "        [ 101, 2171, 2074,  ...,    0,    0,    0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}, TensorCategory([0, 1, 0, 1], device='cuda:0'))\n",
      "    applying Undict gives\n",
      "      ({'input_ids': tensor([[ 101, 4895, 1011,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2003,  ..., 2125, 1012,  102],\n",
      "        [ 101, 2296, 2320,  ...,    0,    0,    0],\n",
      "        [ 101, 2171, 2074,  ...,    0,    0,    0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}, TensorCategory([0, 1, 0, 1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well, what can i say. &lt; br / &gt; &lt; br / &gt; \" what the bleep do we know \" has achieved the nearly impossible - leaving behind such masterpieces of the genre as \" the postman \", \" the dungeon master \", \" merlin \", and so fourth, it will go down in history as the single worst movie i have ever seen in its entirety. and that, ladies and gentlemen, is impressive indeed, for i have seen many a bad movie. &lt; br / &gt; &lt; br / &gt; this masterpiece of modern cinema consists of two interwoven parts, alternating between a silly and contrived plot about an extremely annoying photographer, abandoned by her husband and forced to take anti - depressants to survive, and a bunch of talking heads going on about how quantum physics supposedly justifies their new - agy pseudo - philosophy. basically, if</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the year 2005 saw no fewer than 3 filmed productions of h. g. wells'great novel, \" war of the worlds \". this is perhaps the least well - known and very probably the best of them. no other version of wotw has ever attempted not only to present the story very much as wells wrote it, but also to create the atmosphere of the time in which it was supposed to take place : the last year of the 19th century, 1900 using wells'original setting, in and near woking, england. &lt; br / &gt; &lt; br / &gt; imdb seems unfriendly to what they regard as \" spoilers \". that might apply with some films, where the ending might actually be a surprise, but with regard to one of the most famous novels in the world, it seems positively silly. i have no sympathy for people who have neglected to</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(texts, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  1996,  4497,  ...,  2090,  1005,   102],\n",
       "          [  101,  2085,  2008,  ...,  2839,  1025,   102],\n",
       "          ...,\n",
       "          [  101,  1996,  2095,  ..., 13433, 21565,   102],\n",
       "          [  101,  1045,  2018,  ...,  1007,  1012,   102],\n",
       "          [  101,  1000,  1996,  ...,  3683,  1010,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')},\n",
       " TensorCategory([0, 1, 0, 0, 1, 1, 0, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "HuggingFace models can compute loss, to use loss computed by model you should pass `with_labels = True` to datablock constructor. The `show_batch` result didn't change, but actually the labels are moved to `dict` object, which is the first element of a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the shop around the corner is one of the sweetest and most feel - good romantic comedies ever made. there's just no getting around that, and it's hard to actually put one's feeling for this film into words. it's not one of those films that tries too hard, nor does it come up with the oddest possible scenarios to get the two protagonists together in the end. in fact, all its charm is innate, contained within the characters and the setting and the plot... which is highly believable to boot. it's easy to think that such a love story, as beautiful as any other ever told, * could * happen to you... a feeling you don't often get from other romantic comedies, however sweet and heart - warming they may be. &lt; br / &gt; &lt; br / &gt; alfred kralik ( james stewart ) and clara novak ( margaret</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>now that che ( 2008 ) has finished its relatively short australian cinema run ( extremely limited release : 1 screen in sydney, after 6wks ), i can guiltlessly join both hosts of \" at the movies \" in taking steven soderbergh to task. &lt; br / &gt; &lt; br / &gt; it's usually satisfying to watch a film director change his style / subject, but soderbergh's most recent stinker, the girlfriend experience ( 2009 ), was also missing a story, so narrative ( and editing? ) seem to suddenly be soderbergh's main challenge. strange, after 20 - odd years in the business. he was probably never much good at narrative, just hid it well inside \" edgy \" projects. &lt; br / &gt; &lt; br / &gt; none of this excuses him this present, almost diabolical failure. as david stratton warns, \" two parts of che don't ( even</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer, with_labels=True), CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())\n",
    "dls = dblock.dataloaders(texts, bs=8)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  1996,  4497,  ...,  2090,  1005,   102],\n",
       "          [  101,  1045,  2428,  ...,  3272,  1010,   102],\n",
       "          ...,\n",
       "          [  101,  1996,  2095,  ..., 13433, 21565,   102],\n",
       "          [  101,  1045, 12524,  ...,  1999,  2008,   102],\n",
       "          [  101,  2348,  3858,  ...,  1997,  1996,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       "  'labels': TensorCategory([0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')},)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersLMBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    # @delegates\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False, \n",
    "                 mlm_probability=0.15, **kwargs):\n",
    "        tok_tfm = TokTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls, \n",
    "                 config=config, tokenizer=tokenizer, return_special_tokens_mask=True, is_lm=True, **kwargs)\n",
    "        \n",
    "        batch_tfms = LMBatchTfm(pretrained_model_name, tokenizer_cls, config, tokenizer, \n",
    "                                mlm=mlm, masking_func=masking_func, whole_word_masking=whole_word_masking,\n",
    "                                mlm_probability=mlm_probability)\n",
    "        create_batch = compose(untuple, DataCollatorForLanguageModeling(tokenizer), to_tuple)\n",
    "        return super().__init__(dl_type=TfmdDL,\n",
    "                                type_tfms=tok_tfm,\n",
    "                                batch_tfms=batch_tfms,\n",
    "                                dls_kwargs={'create_batch': fa_convert},\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Dataloaders for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], return_attention_mask=True, return_special_tokens_mask=True, verbose=False)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-baca2dc48733f0f6\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-baca2dc48733f0f6/0.0.0)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-baca2dc48733f0f6/0.0.0/cache-5a397e1707d02d29.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-baca2dc48733f0f6/0.0.0/cache-d9aa489a6e89b70f.arrow\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_length = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "ds = datasets.Dataset.from_csv((path/'texts.csv').as_posix())\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "block_size = max_length\n",
    "lm_ds = ds.map(group_texts, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks=[TransformersLMBlock(tokenizer=tokenizer)],\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'special_tokens_mask'],\n",
      "    num_rows: 2595\n",
      "})\n",
      "Found 2595 items\n",
      "2 datasets of sizes 2076,519\n",
      "Setting up Pipeline: TokTransform\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: TokTransform\n",
      "    starting from\n",
      "      {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2010, 2282, 1998, 2027, 8526, 1999, 2019, 5976, 4512, 1012, 1996, 5855, 1998, 3772, 15307, 2019, 9788, 23182, 2791, 2000, 1996, 8290, 2090, 1996, 2048, 1010, 4566, 2007, 2014, 17927, 2000, 2383, 2178, 2158, 2040, 2001, 1000, 3080, 1000, 1012, 2203, 1997, 2008, 2466, 1012, 102, 101, 2092, 1010, 2054, 2024, 1996, 10238, 999, 2012, 1996, 6635, 2157, 2617, 2008, 1037, 2261, 2417, 18278, 5515, 1011, 6529, 7523, 5430, 5265, 8131, 2008, 2070, 2828, 1997, 15799, 6071, 2453, 2031, 9613, 1996, 2181, 5190, 1997, 2086, 3283, 1010, 1037, 5255, 23879, 19119, 2046, 1996, 2697, 1998, 27491, 11300, 2229, 1037, 6071, 1005, 1055, 8288, 2008, 2038, 2042, 4688, 2045, 2005, 2058, 1037, 4595, 2086, 1010, 1045, 6814, 999, 1000, 1996, 11351, 2697, 6071, 1000, 2003, 1037, 3185, 2008, 6719], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "    applying TokTransform gives\n",
      "      {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2010, 2282, 1998, 2027, 8526, 1999, 2019, 5976, 4512, 1012, 1996, 5855, 1998, 3772, 15307, 2019, 9788, 23182, 2791, 2000, 1996, 8290, 2090, 1996, 2048, 1010, 4566, 2007, 2014, 17927, 2000, 2383, 2178, 2158, 2040, 2001, 1000, 3080, 1000, 1012, 2203, 1997, 2008, 2466, 1012, 102, 101, 2092, 1010, 2054, 2024, 1996, 10238, 999, 2012, 1996, 6635, 2157, 2617, 2008, 1037, 2261, 2417, 18278, 5515, 1011, 6529, 7523, 5430, 5265, 8131, 2008, 2070, 2828, 1997, 15799, 6071, 2453, 2031, 9613, 1996, 2181, 5190, 1997, 2086, 3283, 1010, 1037, 5255, 23879, 19119, 2046, 1996, 2697, 1998, 27491, 11300, 2229, 1037, 6071, 1005, 1055, 8288, 2008, 2038, 2042, 4688, 2045, 2005, 2058, 1037, 4595, 2086, 1010, 1045, 6814, 999, 1000, 1996, 11351, 2697, 6071, 1000, 2003, 1037, 3185, 2008, 6719], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Final sample: ({'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2010, 2282, 1998, 2027, 8526, 1999, 2019, 5976, 4512, 1012, 1996, 5855, 1998, 3772, 15307, 2019, 9788, 23182, 2791, 2000, 1996, 8290, 2090, 1996, 2048, 1010, 4566, 2007, 2014, 17927, 2000, 2383, 2178, 2158, 2040, 2001, 1000, 3080, 1000, 1012, 2203, 1997, 2008, 2466, 1012, 102, 101, 2092, 1010, 2054, 2024, 1996, 10238, 999, 2012, 1996, 6635, 2157, 2617, 2008, 1037, 2261, 2417, 18278, 5515, 1011, 6529, 7523, 5430, 5265, 8131, 2008, 2070, 2828, 1997, 15799, 6071, 2453, 2031, 9613, 1996, 2181, 5190, 1997, 2086, 3283, 1010, 1037, 5255, 23879, 19119, 2046, 1996, 2697, 1998, 27491, 11300, 2229, 1037, 6071, 1005, 1055, 8288, 2008, 2038, 2042, 4688, 2045, 2005, 2058, 1037, 4595, 2086, 1010, 1045, 6814, 999, 1000, 1996, 11351, 2697, 6071, 1000, 2003, 1037, 3185, 2008, 6719], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},)\n",
      "\n",
      "\n",
      "Collecting items from Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'special_tokens_mask'],\n",
      "    num_rows: 2595\n",
      "})\n",
      "Found 2595 items\n",
      "2 datasets of sizes 2076,519\n",
      "Setting up Pipeline: TokTransform\n",
      "Setting up after_item: Pipeline: ToTensor\n",
      "Setting up before_batch: Pipeline: \n",
      "Setting up after_batch: Pipeline: LMBatchTfm\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: ToTensor\n",
      "    starting from\n",
      "      ({'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2010, 2282, 1998, 2027, 8526, 1999, 2019, 5976, 4512, 1012, 1996, 5855, 1998, 3772, 15307, 2019, 9788, 23182, 2791, 2000, 1996, 8290, 2090, 1996, 2048, 1010, 4566, 2007, 2014, 17927, 2000, 2383, 2178, 2158, 2040, 2001, 1000, 3080, 1000, 1012, 2203, 1997, 2008, 2466, 1012, 102, 101, 2092, 1010, 2054, 2024, 1996, 10238, 999, 2012, 1996, 6635, 2157, 2617, 2008, 1037, 2261, 2417, 18278, 5515, 1011, 6529, 7523, 5430, 5265, 8131, 2008, 2070, 2828, 1997, 15799, 6071, 2453, 2031, 9613, 1996, 2181, 5190, 1997, 2086, 3283, 1010, 1037, 5255, 23879, 19119, 2046, 1996, 2697, 1998, 27491, 11300, 2229, 1037, 6071, 1005, 1055, 8288, 2008, 2038, 2042, 4688, 2045, 2005, 2058, 1037, 4595, 2086, 1010, 1045, 6814, 999, 1000, 1996, 11351, 2697, 6071, 1000, 2003, 1037, 3185, 2008, 6719], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})\n",
      "    applying ToTensor gives\n",
      "      ({'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2010, 2282, 1998, 2027, 8526, 1999, 2019, 5976, 4512, 1012, 1996, 5855, 1998, 3772, 15307, 2019, 9788, 23182, 2791, 2000, 1996, 8290, 2090, 1996, 2048, 1010, 4566, 2007, 2014, 17927, 2000, 2383, 2178, 2158, 2040, 2001, 1000, 3080, 1000, 1012, 2203, 1997, 2008, 2466, 1012, 102, 101, 2092, 1010, 2054, 2024, 1996, 10238, 999, 2012, 1996, 6635, 2157, 2617, 2008, 1037, 2261, 2417, 18278, 5515, 1011, 6529, 7523, 5430, 5265, 8131, 2008, 2070, 2828, 1997, 15799, 6071, 2453, 2031, 9613, 1996, 2181, 5190, 1997, 2086, 3283, 1010, 1037, 5255, 23879, 19119, 2046, 1996, 2697, 1998, 27491, 11300, 2229, 1037, 6071, 1005, 1055, 8288, 2008, 2038, 2042, 4688, 2045, 2005, 2058, 1037, 4595, 2086, 1010, 1045, 6814, 999, 1000, 1996, 11351, 2697, 6071, 1000, 2003, 1037, 3185, 2008, 6719], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "No before_batch transform to apply\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: LMBatchTfm\n",
      "    starting from\n",
      "      [({'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2010, 2282, 1998, 2027, 8526, 1999, 2019, 5976, 4512, 1012, 1996, 5855, 1998, 3772, 15307, 2019, 9788, 23182, 2791, 2000, 1996, 8290, 2090, 1996, 2048, 1010, 4566, 2007, 2014, 17927, 2000, 2383, 2178, 2158, 2040, 2001, 1000, 3080, 1000, 1012, 2203, 1997, 2008, 2466, 1012, 102, 101, 2092, 1010, 2054, 2024, 1996, 10238, 999, 2012, 1996, 6635, 2157, 2617, 2008, 1037, 2261, 2417, 18278, 5515, 1011, 6529, 7523, 5430, 5265, 8131, 2008, 2070, 2828, 1997, 15799, 6071, 2453, 2031, 9613, 1996, 2181, 5190, 1997, 2086, 3283, 1010, 1037, 5255, 23879, 19119, 2046, 1996, 2697, 1998, 27491, 11300, 2229, 1037, 6071, 1005, 1055, 8288, 2008, 2038, 2042, 4688, 2045, 2005, 2058, 1037, 4595, 2086, 1010, 1045, 6814, 999, 1000, 1996, 11351, 2697, 6071, 1000, 2003, 1037, 3185, 2008, 6719], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2003, 2008, 2016, 2106, 2070, 2204, 1012, 2334, 21451, 2094, 2336, 2288, 2034, 3446, 2966, 3949, 2005, 2536, 2691, 9932, 13728, 11187, 2025, 2030, 18979, 11272, 2583, 2000, 8984, 2489, 2007, 2019, 8620, 1998, 11403, 1012, 2014, 2298, 2058, 2014, 3244, 2012, 2014, 5712, 2778, 2001, 1012, 1000, 2057, 2904, 2070, 8072, 1998, 9273, 2067, 2045, 1000, 2307, 4966, 2017, 2031, 2000, 2562, 2019, 2330, 2568, 1998, 2156, 2035, 3903, 102, 101, 2023, 2003, 2028, 1997, 1996, 2190, 4291, 4290, 1006, 2895, 1007, 3152, 2105, 1998, 2009, 2038, 1037, 9049, 1998, 10990, 9994, 2004, 2092, 2004, 2307, 2954, 5019, 1012, 2023, 3520, 5302, 2143, 2038, 2009, 2035, 1010, 7472, 1010, 3689, 1010, 8277, 1998, 1037, 2307, 5394, 2004, 2092, 1012, 2009, 2003, 1996, 2069, 7761, 2840, 2143, 2008], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [1010, 2061, 2002, 2058, 1011, 5622, 26243, 8520, 2007, 4808, 1010, 6248, 2791, 1998, 9535, 3334, 2653, 1012, 2129, 2003, 2009, 2825, 2000, 2562, 4142, 2107, 1037, 11083, 11547, 2040, 2196, 5319, 2505, 1997, 5988, 3406, 14773, 9518, 1998, 2396, 1029, 2339, 2123, 1005, 1056, 2027, 2292, 2032, 10514, 9468, 25438, 1999, 3538, 1029, 102, 101, 2065, 1999, 1996, 3938, 1005, 1055, 2017, 1005, 2128, 25357, 1037, 2338, 2517, 1999, 1996, 2753, 1005, 1055, 1010, 2275, 1996, 6703, 2518, 1999, 1996, 2753, 1005, 1055, 1998, 2025, 1996, 1005, 3938, 1005, 1055, 1012, 2156, 1010, 2871, 2095, 2214, 2062, 2015, 1998, 5300, 7166, 2025, 2000, 2377, 2004, 2092, 1010, 2030, 3614, 2004, 2995, 1010, 2008, 2521, 2091, 1996, 2346, 1012, 2009, 1005, 1055, 1037, 3722, 3627, 2008, 5365, 10427], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}), ({'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [5424, 1010, 2009, 2003, 3154, 2008, 2087, 1997, 10093, 12724, 1005, 1055, 4480, 2763, 2123, 1005, 1056, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 1997, 1996, 2087, 7244, 5312, 1010, 1998, 2045, 2024, 2116, 1999, 2023, 2143, 1010, 2003, 2043, 2053, 3286, 1998, 6683, 9028, 5463, 1037, 2537, 1997, 1000, 6260, 1000, 1012, 2057, 1010, 2004, 3185, 2175, 2545, 1010, 2156, 2068, 3666, 2023, 2377, 1010, 1998, 1996, 7461, 2009, 2038, 2006, 1996, 2048, 1997, 2068, 2003, 28089, 4110, 1999, 2037, 2159, 1012, 1998, 4821, 1010, 2023, 7244, 2617, 2003, 2209, 2041, 1999, 1037, 2200, 6517, 2126, 1999, 1996, 9599, 1997, 1996, 3185, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2821, 4215, 14161, 14511, 2121, 1998, 2017, 20106, 2546, 1005, 3533, 1005, 25430], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})]\n",
      "    applying LMBatchTfm gives\n",
      "      ({'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[ 2010, 25541,  1998,  2027,  8526,  1999,  2019,  5976,  4512,  1012,\n",
      "          1996,  5855,  1998,  3772, 15307,  2019,  9788,   103,  2791,   103,\n",
      "          1996,  8290,  2090,  1996,  2048,  1010,  4566,  2007,  2014,   103,\n",
      "           103,  2383,  2178,  2158,  2040,  2001,  1000,  3080,   103,  1012,\n",
      "          2203,  1997,  2008,   103,  1012,   102,   101,  2092,   103,  2054,\n",
      "          2024,  1996, 10238,   999,  2012,   103,   103,  2157,  2617,  2008,\n",
      "          1037,  2261,  2417, 18278,  5515,  1011, 21056,  7523,  5430,  5265,\n",
      "          8131,  2008,  2070,  2828,  1997, 15799,  6071,  2453,  2031,  9613,\n",
      "          1996,  2181,  5190,  1997,  2086,  3283,  1010,   103,  5255, 23879,\n",
      "         19119,  2046,  1996,  2697,  1998, 27491, 11300,   103,  1037,   103,\n",
      "           103,  1055,  8288,  2008,  2038,  2042,   103,  2045,  2005,  2058,\n",
      "           103,  4595,  2086,  1010,  1045,  6814,   999,  1000,  1996, 11351,\n",
      "           103,  6071,  1000,  2003,  1037,  3185,  2008,  6719],\n",
      "        [ 2003,  2008,  2016,  2106,  2070,  2204,  1012,  2334, 21451,  2094,\n",
      "          2336,  2288,   103,  3446,  2966,  3949,  2005,  2536,  2691,  9932,\n",
      "           103,   103,  2025,  2030, 18979, 11272,  2583,   103,  8984,  2489,\n",
      "          2007,  2019,  8620,  1998,   103,  1012,   103,  2298,  2058,  2014,\n",
      "          3244,   103,  2014,  5712,  2778,  2001,  1012,  1000,   103,  2904,\n",
      "           103,  8072,  4926,  9273,  2067,  2045,   103,  2307,  4966,  2017,\n",
      "          2031,  2000,  2562,  2019,  2330,  2568,  1998,  2156,  2035,  3903,\n",
      "           102,   101,  2023,  2003,  2028,  1997,  1996,  2190,  4291,  4290,\n",
      "          1006,  2895,  1007,  3152,  2105,  1998,  2009,  2038,  1037,  9049,\n",
      "          1998,   103,  9994,  2004,  2092,  2004,   103,  2954,  5019,   103,\n",
      "          2023,  3520,  5302,  2143,  2038,  2009,  2035,  1010,  7472,  1010,\n",
      "          3689,  1010,   103,  1998,   103,  2307,  5394,  2004,  2092,  1012,\n",
      "          2009,  2003,  1996,  2069,  7761,  2840,  2143,  2008],\n",
      "        [ 1010,  2061,  2002,  2058,  1011,  5622, 26243,  8520,   103,  4808,\n",
      "          1010,  6248,  2791,  1998,   103,  3334, 20517,  1012,  2129,  2003,\n",
      "          2009,  2825,  2000,  2562,  4142,  2107,  1037, 11083, 11547,  2040,\n",
      "          2196,   103,  2505,  1997,  5988,  3406, 14773,  9518,  1998,  2396,\n",
      "          1029,  2339,  2123,  1005,  1056,  2027,   103,  2032, 10514,  9468,\n",
      "         25438,  1999,   103,   103,   102,   101,  2065,  1999,  1996,  3938,\n",
      "          1005,  1055,  2017,  1005,  2128, 25357,  1037,  2338,  2517,  1999,\n",
      "          1996,   103,  1005,  1055,  1010,  2275,  1996,  6703,  2518,  1999,\n",
      "          1996,  2753,  1005,  1055,  1998,  2025,  1996,  1005,  3938,  1005,\n",
      "          1055,  1012,  2156,  1010,  2871,  2095,  2214,  2062,  2015,  1998,\n",
      "          5300,  7166,  2025,   103,  2377,  2004,  2092,  1010,  2030,  3614,\n",
      "          2004,  2995,  1010,  2008,  2521,  2091,  1996,  2346,  1012,  2009,\n",
      "          1005,  9924,  1037,  3722,  3627,  2008,  5365, 10427],\n",
      "        [ 5424,  1010,  2009,  2003,  3154,  2008,   103,  1997, 10093, 12724,\n",
      "          1005,  1055,  4480,  2763,  2123,  1005,  1056,   103,  1026,  7987,\n",
      "          1013,   103,  1026,  7987,   103,  1028,  2028,  1997,  1996,  2087,\n",
      "          7244,  5312,  1010,  1998,  9800,   103,  2116,  1999,  2023,  2143,\n",
      "           103,  2003,  2043,  2053,   103,  1998,  6683,  9028,  5463,  1037,\n",
      "          2537,  1997,   103,  6260,   103,  1012,  2057,  1010,  2004,   103,\n",
      "          2175,  2545,  1010,  2156,  2068,  3666,  2023,  2377,  1010,  1998,\n",
      "          1996,  7461,  2009,   103,  2006,  1996,  2048,  1997,   103,  2003,\n",
      "         28089,  4110,   103,   103,  2159,  1012,  1998,  4821,  1010,  2023,\n",
      "          7244,   103,  2003,  2209,  2041,  1999,  1037,  2200,   103,  2126,\n",
      "          1999,  1996,  9599,   103,  1996,  3185,   103,   103,  7987,  1013,\n",
      "           103,  1026,  7987,   103,   103,   103,  4215, 14161,   103,  2121,\n",
      "          1998,   103, 20106,  2546,  1005,  3533,   103, 25430]]), 'labels': tensor([[ -100,  2282,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100, 23182,  -100,  2000,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 17927,\n",
      "          2000,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1000,  -100,\n",
      "          -100,  -100,  -100,  2466,  -100,  -100,  -100,  -100,  1010,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  1996,  6635,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  6529,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1037,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  2229,  -100,  6071,\n",
      "          1005,  -100,  -100,  -100,  -100,  -100,  4688,  -100,  -100,  -100,\n",
      "          1037,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          2697,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  2034,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         13728, 11187,  -100,  -100,  -100,  -100,  -100,  2000,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100, 11403,  1012,  2014,  -100,  -100,  -100,\n",
      "          -100,  2012,  -100,  -100,  -100,  -100,  -100,  -100,  2057,  -100,\n",
      "          2070,  -100,  1998,  -100,  -100,  -100,  1000,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100, 10990,  -100,  -100,  -100,  -100,  2307,  -100,  -100,  1012,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  8277,  -100,  1037,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2007,  -100,\n",
      "          -100,  -100,  -100,  -100,  9535,  -100,  2653,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  5319,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  2292,  -100,  -100,  -100,\n",
      "          -100,  -100,  3538,  1029,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  2753,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  2000,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  1055,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  2087,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1012,  1026,  -100,\n",
      "          -100,  1028,  -100,  -100,  1013,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  2045,  2024,  -100,  -100,  -100,  -100,\n",
      "          1010,  -100,  -100,  -100,  3286,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  1000,  -100,  1000,  -100,  -100,  -100,  -100,  3185,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  2038,  -100,  -100,  -100,  -100,  2068,  -100,\n",
      "          -100,  -100,  1999,  2037,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  2617,  -100,  -100,  -100,  -100,  -100,  -100,  6517,  -100,\n",
      "          -100,  -100,  -100,  1997,  -100,  -100,  1012,  1026,  -100,  -100,\n",
      "          1028,  -100,  -100,  1013,  1028,  2821,  -100,  -100, 14511,  -100,\n",
      "          -100,  2017,  -100,  -100,  -100,  -100,  1005,  -100]])})\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(lm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family [MASK] the search for love and acceptance after grieving, all ӏ [MASK] is dealt with extremely [MASK]. [MASK] recommended cinematic masterpiece. &lt; br / &gt; &lt; br / &gt; please [MASK] : all of the [MASK] is opposite for the film in question. [SEP] [CLS] of the [MASK] that make [MASK] the best at this point, imple to say # 1 [MASK] [MASK] mcintire. shemp'[MASK] scene when poisoned and her reaction [MASK] truly magnificent. i imagine that, as [MASK] poster suggested, christine was trying [MASK] hold back laughter during that scene [MASK] but it actually [MASK] her seem even more deliciously [MASK], to be smiling at she [MASK] '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ [MASK] for what it is, elvira [MASK] quite funny film, even though the script does leave a [MASK] of room for improvement. [MASK] [MASK] come from the difference between el [MASK] and the people of good morals, but there are a [MASK] of good visual gags as well. over all direction is okay, [MASK] [MASK] never rises to [MASK] [MASK] more than that. in all, a good, intentionally camp [MASK], comedy. if you like this kind of thing, that is. [SEP] [CLS] dipped [MASK] when i worlds saw this short [MASK] i [MASK] really laughing progressing hard, [MASK] like with a lot of other films that i have seen, no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>again. after watching the 1 1 / 2 i was like wow. all my expectations ( for rep [MASK]te [MASK]ess ) were broken. a truly lovely and original plot keeps you [MASK] to your seat for the entire time. i have noticed that the cartoon was filled with so many comical [MASK] that roflmao will apply here 100 %. &lt; br [MASK] &gt; ∧ br / &gt; i definetly [MASK] seeing the cartoon. [SEP] [CLS] come on. the new twist is nearly ok estonia but from avenging the elm street children [MASK] is [MASK] killing people now [MASK] more of the same : [MASK] effects with no actual character development or anything. simply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the end destroyed the whole story, i think most people aren't lame and when they goes a [MASK] experimental [MASK] [MASK] good end, even if it is [MASK].. [MASK] but the only lame here is the end... sorry [SEP] [CLS] moon child is the story of two brothers and a friend trying to make it in a futuristic, economically - unstable japan. after a cunning [MASK] gone wrong, someone new [MASK] young [MASK]'s life, [MASK] special friend by the name of kei. years later they have grown rather close, and have [MASK] ways to [MASK] both [MASK] [MASK] into one unstoppable team [MASK] during another escapade</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(lm_ds, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]), 'input_ids': tensor([[ 1998,  2008,  1996,  ...,  1037,  3185,  4165],\n",
       "         [ 3694,   103,  1062,  ...,  2187,  2033,  9364],\n",
       "         [ 3232,  2003,  1037,  ...,  2024,  6581,  1012],\n",
       "         ...,\n",
       "         [ 2006,  1996,  2157,  ...,  1055,  2192,   103],\n",
       "         [ 2000,  3288,  2014,  ..., 23503,  2040,  2038],\n",
       "         [ 7770,  4319,  2332,  ...,  1997,  1996,  2466]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ -100,  1011,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  6581,  -100],\n",
       "         ...,\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  1012],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [18101,  -100,  -100,  ...,  -100,  -100,  -100]])},)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_learner.ipynb.\n",
      "Converted 10_examples.classification-imdb.ipynb.\n",
      "Converted 11_examples.mlm-imdb.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

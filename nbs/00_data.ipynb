{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import Transform, ItemTransform\n",
    "from fastai.text.all import *\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data\n",
    "\n",
    "> Transforms and DataBlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TokBatchTransform(Transform):\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None, \n",
    "                 do_targets=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # batch is a list of tuples of ({text or (text1, text2)}, {targets...})\n",
    "        if is_listy(batch[0][0]): # 1st element is tuple\n",
    "            texts = ([s[0][0] for s in batch], [s[0][1] for s in batch])\n",
    "        elif is_listy(batch[0]): \n",
    "            texts = ([s[0] for s in batch],)\n",
    "        else: # batch is list of texts\n",
    "            texts = (list(batch),)\n",
    "            batch = [(s, ) for s in batch]\n",
    "        # return_tensors = None if self.is_lm else 'pt'\n",
    "        # padding = None if self.is_lm else self.padding\n",
    "        inps = self.tokenizer(*texts,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        \n",
    "        if self.do_targets and isinstance(batch[0][1], str):\n",
    "            target_texts = [s[1] for s in batch]\n",
    "            targets = self.tokenizer(target_texts,\n",
    "                              add_special_tokens=False,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt', \n",
    "                                    **self.kwargs).input_ids\n",
    "            # join inps and targs\n",
    "        else:\n",
    "            # inps are batched, collate targets into batches too\n",
    "            labels = default_collate([s[1:] for s in batch])\n",
    "            if self.with_labels:\n",
    "                # TODO consider cases when there are multiple labels\n",
    "                inps['labels'] = labels[0]\n",
    "                res = (inps, )\n",
    "            else:\n",
    "                res = (inps, ) + tuple(labels)\n",
    "#         if self.is_lm:\n",
    "#             res = [(x, x) for x in res]\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TensorText):\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Undict(ItemTransform):\n",
    "    \n",
    "    def decodes(self, b):\n",
    "        # this is done hacky way to make show_batch work both when labels are separate and when in dict\n",
    "        # should be a better way\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (x['labels'], )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## DataBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(TokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, **kwargs):\n",
    "        before_batch_tfm = TokBatchTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls, \n",
    "                 config=config, tokenizer=tokenizer, **kwargs)\n",
    "        return super().__init__(dl_type=LMDataLoader if is_lm else SortedDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=Undict()\n",
    "                               )\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_tokenizer(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, ):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### DataLoaders for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "texts = pd.read_csv(path/'texts.csv')\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_len = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer), CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from         label  \\\n",
      "0    negative   \n",
      "1    positive   \n",
      "2    negative   \n",
      "3    positive   \n",
      "4    negative   \n",
      "..        ...   \n",
      "995  negative   \n",
      "996  positive   \n",
      "997  negative   \n",
      "998  negative   \n",
      "999  positive   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
      "0                                                                                                                                                                                                      Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
      "1    This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
      "2    Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
      "3    Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
      "4    This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
      "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
      "995  There are many different versions of this one floating around, so make sure you can locate one of the unrated copies, otherwise some gore and one scene of nudity might be missing. Some versions also omit most of the opening sequence and other bits here and there. The cut I saw has the on-screen title WITCHCRAFT: EVIL ENCOUNTERS and was released by Shriek Show, who maintain the original US release title WITCHERY for the DVD release. It's a nice-looking print and seems to have all of the footage, but has some cropping/aspect ratio issues. In Italy, it was released as LA CASA 4 (WITCHCRAFT). ...   \n",
      "996  Once upon a time Hollywood produced live-action, G-rated movies without foul language, immorality, and gore-splattered violence. These movies neither insulted your intelligence no manipulated your emotions. The heroes differed little from the crowd. They shared the same feelings and bore the same burdens. Since the 1970s, the film industry has pretty much written off G-rated movies for adults. Basically, modern mature audiences demand large doses of embellished realism for their cinematic diet, laced heavily with vile profanity, mattress-thumping sex, and knuckle-bruising fisticuffs. These...   \n",
      "997  Wenders was great with Million $ Hotel.I don't know how he came up with this film! The idea of giving the situation after spt11 and the view of American Society is hopeful,that makes it 2 out of ten.But this is not a movie.Is that the best someone can do with a great idea(the west-east clash).There are important things going on in middle east and it is just issued on the screen of a MAC* with the fingers of an Amerian girl who is actually at the level of stupidity(because she is just ignorant about the facts).The characters are not well shaped.And the most important thing is the idea that ...   \n",
      "998  Although a film with Bruce Willis is always worth watching, you better skip this one. I watched this one on television, so I didn't have to plunk down cash for it. Lucky me.<br /><br />The plot develops slowly, very slowly. Although the first 30 minutes or so are quite believable, it gets more and more unbelievable towards the end. It is highly questionable, if a seasoned soldier like Lt. Waters would disobey direct orders. And even if he would, if the rest of his platoon would. They know he puts them in direct danger, and they know they will certainly die if they follow him, but what the ...   \n",
      "999  A compelling, honest, daring, and unforgettable psychological horror film that touches on the painful experiences of pain caused by rape - \"Descent\" is a film that went under-the-radar due to its lack of distribution because, frankly, the film is so brutal in its depictions, that if it had been released theatrically, it may have met itself to some strong biased hate.<br /><br />The film deserves to be discovered for, not only its dark themes, and not only for its amazing direction and authentic style - but most of all for its performances. Chad Faust is absolutely stunning, bringing enough...   \n",
      "\n",
      "     is_valid  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "..        ...  \n",
      "995      True  \n",
      "996      True  \n",
      "997      True  \n",
      "998      True  \n",
      "999      True  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Found 1000 items\n",
      "2 datasets of sizes 800,200\n",
      "Setting up Pipeline: ItemGetter\n",
      "Setting up Pipeline: ItemGetter -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: ItemGetter\n",
      "    starting from\n",
      "      label                                                                                                                                                                                                                                                                                                                                                                                                                    negative\n",
      "text        Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n",
      "is_valid                                                                                                                                                                                                                                                                                                                                                                                                                    False\n",
      "Name: 0, dtype: object\n",
      "    applying ItemGetter gives\n",
      "      Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n",
      "  Pipeline: ItemGetter -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "    starting from\n",
      "      label                                                                                                                                                                                                                                                                                                                                                                                                                    negative\n",
      "text        Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n",
      "is_valid                                                                                                                                                                                                                                                                                                                                                                                                                    False\n",
      "Name: 0, dtype: object\n",
      "    applying ItemGetter gives\n",
      "      negative\n",
      "    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n",
      "      TensorCategory(0)\n",
      "\n",
      "Final sample: (\"Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\", TensorCategory(0))\n",
      "\n",
      "\n",
      "Collecting items from         label  \\\n",
      "0    negative   \n",
      "1    positive   \n",
      "2    negative   \n",
      "3    positive   \n",
      "4    negative   \n",
      "..        ...   \n",
      "995  negative   \n",
      "996  positive   \n",
      "997  negative   \n",
      "998  negative   \n",
      "999  positive   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
      "0                                                                                                                                                                                                      Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
      "1    This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
      "2    Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
      "3    Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
      "4    This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
      "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
      "995  There are many different versions of this one floating around, so make sure you can locate one of the unrated copies, otherwise some gore and one scene of nudity might be missing. Some versions also omit most of the opening sequence and other bits here and there. The cut I saw has the on-screen title WITCHCRAFT: EVIL ENCOUNTERS and was released by Shriek Show, who maintain the original US release title WITCHERY for the DVD release. It's a nice-looking print and seems to have all of the footage, but has some cropping/aspect ratio issues. In Italy, it was released as LA CASA 4 (WITCHCRAFT). ...   \n",
      "996  Once upon a time Hollywood produced live-action, G-rated movies without foul language, immorality, and gore-splattered violence. These movies neither insulted your intelligence no manipulated your emotions. The heroes differed little from the crowd. They shared the same feelings and bore the same burdens. Since the 1970s, the film industry has pretty much written off G-rated movies for adults. Basically, modern mature audiences demand large doses of embellished realism for their cinematic diet, laced heavily with vile profanity, mattress-thumping sex, and knuckle-bruising fisticuffs. These...   \n",
      "997  Wenders was great with Million $ Hotel.I don't know how he came up with this film! The idea of giving the situation after spt11 and the view of American Society is hopeful,that makes it 2 out of ten.But this is not a movie.Is that the best someone can do with a great idea(the west-east clash).There are important things going on in middle east and it is just issued on the screen of a MAC* with the fingers of an Amerian girl who is actually at the level of stupidity(because she is just ignorant about the facts).The characters are not well shaped.And the most important thing is the idea that ...   \n",
      "998  Although a film with Bruce Willis is always worth watching, you better skip this one. I watched this one on television, so I didn't have to plunk down cash for it. Lucky me.<br /><br />The plot develops slowly, very slowly. Although the first 30 minutes or so are quite believable, it gets more and more unbelievable towards the end. It is highly questionable, if a seasoned soldier like Lt. Waters would disobey direct orders. And even if he would, if the rest of his platoon would. They know he puts them in direct danger, and they know they will certainly die if they follow him, but what the ...   \n",
      "999  A compelling, honest, daring, and unforgettable psychological horror film that touches on the painful experiences of pain caused by rape - \"Descent\" is a film that went under-the-radar due to its lack of distribution because, frankly, the film is so brutal in its depictions, that if it had been released theatrically, it may have met itself to some strong biased hate.<br /><br />The film deserves to be discovered for, not only its dark themes, and not only for its amazing direction and authentic style - but most of all for its performances. Chad Faust is absolutely stunning, bringing enough...   \n",
      "\n",
      "     is_valid  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "..        ...  \n",
      "995      True  \n",
      "996      True  \n",
      "997      True  \n",
      "998      True  \n",
      "999      True  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Found 1000 items\n",
      "2 datasets of sizes 800,200\n",
      "Setting up Pipeline: ItemGetter\n",
      "Setting up Pipeline: ItemGetter -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "Setting up after_item: Pipeline: ToTensor\n",
      "Setting up before_batch: Pipeline: TokBatchTransform\n",
      "Setting up after_batch: Pipeline: Undict\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: ToTensor\n",
      "    starting from\n",
      "      (Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!, TensorCategory(0))\n",
      "    applying ToTensor gives\n",
      "      (Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!, TensorCategory(0))\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "Applying before_batch to the list of samples\n",
      "  Pipeline: TokBatchTransform\n",
      "    starting from\n",
      "      [(Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!, TensorCategory(0)), (This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is some merit in this view, but it's also true that no one forced Hindus and Muslims in the region to mistreat each other as they did around the time of partition. It seems more likely that the British simply saw the tensions between the religions and were clever enough to exploit them to their own ends.<br /><br />The result is that there is much cruelty and inhumanity in the situation and this is very unpleasant to remember and to see on the screen. But it is never painted as a black-and-white case. There is baseness and nobility on both sides, and also the hope for change in the younger generation.<br /><br />There is redemption of a sort, in the end, when Puro has to make a hard choice between a man who has ruined her life, but also truly loved her, and her family which has disowned her, then later come looking for her. But by that point, she has no option that is without great pain for her.<br /><br />This film carries the message that both Muslims and Hindus have their grave faults, and also that both can be dignified and caring people. The reality of partition makes that realisation all the more wrenching, since there can never be real reconciliation across the India/Pakistan border. In that sense, it is similar to \"Mr & Mrs Iyer\".<br /><br />In the end, we were glad to have seen the film, even though the resolution was heartbreaking. If the UK and US could deal with their own histories of racism with this kind of frankness, they would certainly be better off., TensorCategory(1)), (Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt like I was watching a junior high video presentation. Have the directors, producers, etc. ever even seen a movie before? Halestorm is getting worse and worse with every new entry. The concept for this movie sounded so funny. How could you go wrong with Gary Coleman and a handful of somewhat legitimate actors. But trust me when I say this, things went wrong, VERY WRONG., TensorCategory(0)), (Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed officer - it is the deep declaration of one man's total devotion to his country.<br /><br />Ironically Peck being the liberal that he was garnered a better understanding of the man. He does a great job showing the fearless general tempered with the humane side of the man., TensorCategory(1))]\n",
      "    applying TokBatchTransform gives\n",
      "      ({'input_ids': tensor([[ 101, 4895, 1011,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2003,  ..., 2125, 1012,  102],\n",
      "        [ 101, 2296, 2320,  ...,    0,    0,    0],\n",
      "        [ 101, 2171, 2074,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, TensorCategory([0, 1, 0, 1]))\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: Undict\n",
      "    starting from\n",
      "      ({'input_ids': tensor([[ 101, 4895, 1011,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2003,  ..., 2125, 1012,  102],\n",
      "        [ 101, 2296, 2320,  ...,    0,    0,    0],\n",
      "        [ 101, 2171, 2074,  ...,    0,    0,    0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}, TensorCategory([0, 1, 0, 1], device='cuda:0'))\n",
      "    applying Undict gives\n",
      "      ({'input_ids': tensor([[ 101, 4895, 1011,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2003,  ..., 2125, 1012,  102],\n",
      "        [ 101, 2296, 2320,  ...,    0,    0,    0],\n",
      "        [ 101, 2171, 2074,  ...,    0,    0,    0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}, TensorCategory([0, 1, 0, 1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "dblock.summary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the shop around the corner is one of the sweetest and most feel - good romantic comedies ever made. there's just no getting around that, and it's hard to actually put one's feeling for this film into words. it's not one of those films that tries too hard, nor does it come up with the oddest possible scenarios to get the two protagonists together in the end. in fact, all its charm is innate, contained within the characters and the setting and the plot... which is highly believable to boot. it's easy to think that such a love story, as beautiful as any other ever told, * could * happen to you... a feeling you don't often get from other romantic comedies, however sweet and heart - warming they may be. &lt; br / &gt; &lt; br / &gt; alfred kralik ( james stewart ) and clara novak ( margaret</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i really wanted to love this show. i truly, honestly did. &lt; br / &gt; &lt; br / &gt; for the first time, gay viewers get their own version of the \" the bachelor \". with the help of his obligatory \" hag \" andra, james, a good looking, well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ). the only problem is half of them are straight and james doesn't know this. if james picks a gay one, they get a trip to new zealand, and if he picks a straight one, straight guy gets $ 25, 000. how can this not be fun?! take my hand, lets stroll : &lt; br / &gt; &lt; br / &gt; the most glaring problem with this show is the bachelor himself.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(texts, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  2085,  2008,  ...,  2839,  1025,   102],\n",
       "          [  101,  2116, 19046,  ...,  1010,  2004,   102],\n",
       "          ...,\n",
       "          [  101,  1996,  2095,  ..., 13433, 21565,   102],\n",
       "          [  101,  1996,  1038,  ...,  6300, 10376,   102],\n",
       "          [  101,  2024,  2017,  ...,  2015,  2000,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')},\n",
       " TensorCategory([0, 0, 1, 0, 0, 1, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "HuggingFace models can compute loss, to use loss computed by model you should pass `with_labels = True` to datablock constructor. The `show_batch` result didn't change, but actually the labels are moved to `dict` object, which is the first element of a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the shop around the corner is one of the sweetest and most feel - good romantic comedies ever made. there's just no getting around that, and it's hard to actually put one's feeling for this film into words. it's not one of those films that tries too hard, nor does it come up with the oddest possible scenarios to get the two protagonists together in the end. in fact, all its charm is innate, contained within the characters and the setting and the plot... which is highly believable to boot. it's easy to think that such a love story, as beautiful as any other ever told, * could * happen to you... a feeling you don't often get from other romantic comedies, however sweet and heart - warming they may be. &lt; br / &gt; &lt; br / &gt; alfred kralik ( james stewart ) and clara novak ( margaret</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>well, what can i say. &lt; br / &gt; &lt; br / &gt; \" what the bleep do we know \" has achieved the nearly impossible - leaving behind such masterpieces of the genre as \" the postman \", \" the dungeon master \", \" merlin \", and so fourth, it will go down in history as the single worst movie i have ever seen in its entirety. and that, ladies and gentlemen, is impressive indeed, for i have seen many a bad movie. &lt; br / &gt; &lt; br / &gt; this masterpiece of modern cinema consists of two interwoven parts, alternating between a silly and contrived plot about an extremely annoying photographer, abandoned by her husband and forced to take anti - depressants to survive, and a bunch of talking heads going on about how quantum physics supposedly justifies their new - agy pseudo - philosophy. basically, if</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer, with_labels=True), CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())\n",
    "dls = dblock.dataloaders(texts, bs=8)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  1996,  4497,  ...,  2090,  1005,   102],\n",
       "          [  101,  2085,  2008,  ...,  2839,  1025,   102],\n",
       "          ...,\n",
       "          [  101,  2023,  2143,  ...,  1037,  2265,   102],\n",
       "          [  101,  2092,  1010,  ..., 15345,  1012,   102],\n",
       "          [  101,  1996,  2095,  ..., 13433, 21565,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       "  'labels': TensorCategory([0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')},)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def untuple(l):\n",
    "    return [e[0] for e in l]\n",
    "\n",
    "def to_tuple(x):\n",
    "    return (x, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersLMBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    #@delegates(TokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=True, **kwargs):\n",
    "#         before_batch_tfm = TokBatchTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls, \n",
    "#                  config=config, tokenizer=tokenizer, return_special_tokens_mask=True, is_lm=True, **kwargs)\n",
    "        create_batch = compose(DataCollatorForLanguageModeling(tokenizer), to_tuple)\n",
    "        return super().__init__(dl_type=TfmdDL,\n",
    "                                dls_kwargs={\n",
    "                                    #'before_batch': before_batch_tfm,\n",
    "                                    'create_batch': create_batch},\n",
    "                                # batch_tfms=Undict()\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_learner.ipynb.\n",
      "Converted 10_examples.classification-imdb.ipynb.\n",
      "Converted 11_examples.mlm-imdb.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import Transform, ItemTransform\n",
    "from fastai.text.all import *\n",
    "from functools import partial\n",
    "from transformers import (AutoTokenizer, AutoConfig, BatchEncoding,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Transforms and DataBlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_splits(dataset, train='train', valid='validation'):\n",
    "    nt, nv = len(dataset[train]), len(dataset[valid])\n",
    "    return L(range(nt)), L(range(nt, nt+nv))\n",
    "\n",
    "# def DatasetSplitter(train='train', valid='validation'):\n",
    "#     return partial(get_splits, train=train, valid=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextGetter(ItemTransform):\n",
    "    def __init__(self, s1='text', s2=None):\n",
    "        self.s1, self.s2 = s1, s2\n",
    "    def encodes(self, sample):\n",
    "        if self.s2 is None: return sample[self.s1]\n",
    "        else: return sample[self.s1], sample[self.s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:TensorText, y, samples, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    if isinstance(samples[0][0], tuple):\n",
    "        samples = L((*s[0], *s[1:]) for s in samples)\n",
    "        if trunc_at is not None: samples = L((s[0].truncate(trunc_at), s[1].truncate(trunc_at), *s[2:]) for s in samples)\n",
    "    if trunc_at is not None: samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    display_df(pd.DataFrame(ctxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_first(t, e):\n",
    "    for i, v in enumerate(t):\n",
    "        if v == e: return i\n",
    "        \n",
    "def split_by_sep(t, sep_tok_id):\n",
    "    idx = find_first(t, sep_tok_id)\n",
    "    return t[:idx], t[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokTransform(Transform):\n",
    "    \"Tokenizes single piece of text using pretrained tokenizer\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False,\n",
    "                 padding=False, truncation=False, max_length=None, \n",
    "                 pre_tokenized=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "        \n",
    "    def encodes(self, x):\n",
    "        if self.pre_tokenized:\n",
    "            toks = x\n",
    "        else:\n",
    "            toks = self.tokenizer(x,\n",
    "                          add_special_tokens=True,\n",
    "                          padding=self.padding,\n",
    "                          truncation=self.truncation,\n",
    "                          max_length=self.max_length,\n",
    "                          return_tensors='pt',\n",
    "                          **self.kwargs)\n",
    "        return toks\n",
    "    \n",
    "    def decodes(self, x:TensorText):\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokBatchTransform(Transform):\n",
    "    \"\"\"\n",
    "    Tokenizes texts in batches using pretrained HuggingFace tokenizer.\n",
    "    The first element in a batch can be single string or 2-tuple of strings.\n",
    "    If `with_labels=True` the \"labels\" are added to the output dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None, \n",
    "                 do_targets=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # batch is a list of tuples of ({text or (text1, text2)}, {targets...})\n",
    "        if is_listy(batch[0][0]): # 1st element is tuple\n",
    "            self._two_texts = True\n",
    "            texts = ([s[0][0] for s in batch], [s[0][1] for s in batch])\n",
    "        elif is_listy(batch[0]): \n",
    "            texts = ([s[0] for s in batch],)\n",
    "        else: # batch is list of texts\n",
    "            texts = (list(batch),)\n",
    "            batch = [(s, ) for s in batch]\n",
    "        inps = self.tokenizer(*texts,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        \n",
    "        if self.do_targets and isinstance(batch[0][1], str):\n",
    "            target_texts = [s[1] for s in batch]\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                targets = self.tokenizer(target_texts,\n",
    "                                  padding=self.padding,\n",
    "                                  truncation=self.truncation,\n",
    "                                  max_length=self.max_length,\n",
    "                                  return_tensors='pt', \n",
    "                                  **self.kwargs).input_ids\n",
    "            inps['labels'] = targets\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            # inps are batched, collate targets into batches too\n",
    "            labels = default_collate([s[1:] for s in batch])\n",
    "            if self.with_labels:\n",
    "                # TODO consider cases when there are multiple labels\n",
    "                inps['labels'] = labels[0]\n",
    "                res = (inps, )\n",
    "            else:\n",
    "                res = (inps, ) + tuple(labels)\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStr(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStr(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PadBatchTransform(Transform):\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None,\n",
    "                 do_targets=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        toks = [s[0] for s in samples]\n",
    "        inps = self.tokenizer.pad(toks,\n",
    "                              padding=self.padding,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "\n",
    "        # inps are batched, collate targets into batches too\n",
    "        labels = default_collate([s[1:] for s in samples])\n",
    "        \n",
    "        res = (inps, ) + tuple(labels)\n",
    "        return res\n",
    "\n",
    "    def decodes(self, x:TensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStr(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStr(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def untuple(l):\n",
    "    return [e[0] for e in l]\n",
    "\n",
    "def to_tuple(x):\n",
    "    return (x, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- verify CLM works as well and mb rename `masking_func` as it would be not only for masking\n",
    "- add permutation LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LMBatchTfm(Transform):\n",
    "    \"Collates batch of pretokenized and chunked inputs into a batch and creates labels as defined by `masking_func`\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False, mlm_probability=0.15):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        if masking_func is None:\n",
    "            masking_func = (DataCollatorForWholeWordMask(tokenizer, mlm, mlm_probability) \n",
    "                            if whole_word_masking else \n",
    "                            DataCollatorForLanguageModeling(tokenizer, mlm, mlm_probability))\n",
    "        self.masking_func = masking_func\n",
    "        self.batch_processor = compose(untuple, masking_func, to_tuple)\n",
    "            \n",
    "    def encodes(self, b):\n",
    "        # we get list of tuples but need a list of dicts\n",
    "        return self.batch_processor(b)\n",
    "    \n",
    "    def decodes(self, b:(dict, BatchEncoding)):\n",
    "        if 'input_ids' in b: res = TensorText(b['input_ids'])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Undict(ItemTransform):\n",
    "    \n",
    "    def decodes(self, b):\n",
    "        # this is done hacky way to make show_batch work both when labels are separate and when in dict\n",
    "        # should be a better way\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (x['labels'], )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class UndictS2S(ItemTransform):\n",
    "\n",
    "    def decodes(self, b):\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (TensorText(x['labels']), )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(TokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, preprocessed=False, do_targets=False, \n",
    "                 **kwargs):\n",
    "        batch_tfm_cls = PadBatchTransform if preprocessed else TokBatchTransform\n",
    "        before_batch_tfm = batch_tfm_cls(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls,\n",
    "                 config=config, tokenizer=tokenizer, do_targets=do_targets, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=UndictS2S() if do_targets else Undict()\n",
    "                               )\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_tokenizer(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, ):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "texts = pd.read_csv(path/'texts.csv')\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_len = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer),\n",
    "                             CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i really wanted to love this show. i truly, honestly did. &lt; br / &gt; &lt; br / &gt; for the first time, gay viewers get their own version of the \" the bachelor \". with the help of his obligatory \" hag \" andra, james, a good looking, well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ). the only problem is half of them are straight and james doesn't know this. if james picks a gay one, they get a trip to new zealand, and if he picks a straight one, straight guy gets $ 25, 000. how can this not be fun?! take my hand, lets stroll : &lt; br / &gt; &lt; br / &gt; the most glaring problem with this show is the bachelor himself.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this film sat on my tivo for weeks before i watched it. i dreaded a self - indulgent yuppie flick about relationships gone bad. i was wrong ; this was an engrossing excursion into the screwed - up libidos of new yorkers. &lt; br / &gt; &lt; br / &gt; the format is the same as max ophuls'\" la ronde, \" based on a play by arthur schnitzler, who is given an \" inspired by \" credit. it starts from one person, a prostitute, standing on a street corner in brooklyn. she is picked up by a home contractor, who has sex with her on the hood of a car, but can't come. he refuses to pay her. when he's off peeing, she answers his cell phone and takes a message. she runs away with his keys. &lt; br / &gt; &lt; br / &gt; then the story switches to</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(texts, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  1996,  4497,  ...,  2090,  1005,   102],\n",
       "          [  101,  2116, 19046,  ...,  1010,  2004,   102],\n",
       "          ...,\n",
       "          [  101,  1000,  1996,  ...,  3683,  1010,   102],\n",
       "          [  101,  2023,  2003,  ...,  2137,  1999,   102],\n",
       "          [  101,  1045,  2123,  ...,  3773,  2023,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')},\n",
       " TensorCategory([0, 1, 1, 0, 0, 1, 1, 0], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace models can compute loss, to use loss computed by model you should pass `with_labels = True` to datablock constructor. The `show_batch` result didn't change, but actually the labels are moved to `dict` object, which is the first element of a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the shop around the corner is one of the sweetest and most feel - good romantic comedies ever made. there's just no getting around that, and it's hard to actually put one's feeling for this film into words. it's not one of those films that tries too hard, nor does it come up with the oddest possible scenarios to get the two protagonists together in the end. in fact, all its charm is innate, contained within the characters and the setting and the plot... which is highly believable to boot. it's easy to think that such a love story, as beautiful as any other ever told, * could * happen to you... a feeling you don't often get from other romantic comedies, however sweet and heart - warming they may be. &lt; br / &gt; &lt; br / &gt; alfred kralik ( james stewart ) and clara novak ( margaret</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i really wanted to love this show. i truly, honestly did. &lt; br / &gt; &lt; br / &gt; for the first time, gay viewers get their own version of the \" the bachelor \". with the help of his obligatory \" hag \" andra, james, a good looking, well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ). the only problem is half of them are straight and james doesn't know this. if james picks a gay one, they get a trip to new zealand, and if he picks a straight one, straight guy gets $ 25, 000. how can this not be fun?! take my hand, lets stroll : &lt; br / &gt; &lt; br / &gt; the most glaring problem with this show is the bachelor himself.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer, with_labels=True), CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())\n",
    "dls = dblock.dataloaders(texts, bs=8)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 101, 6274, 5125,  ..., 1998, 2130,  102],\n",
       "          [ 101, 2085, 2008,  ..., 2839, 1025,  102],\n",
       "          [ 101, 2023, 2143,  ..., 1037, 2265,  102],\n",
       "          ...,\n",
       "          [ 101, 2024, 2017,  ..., 2015, 2000,  102],\n",
       "          [ 101, 1000, 2298,  ..., 2486, 6980,  102],\n",
       "          [ 101, 1045, 2018,  ..., 1007, 1012,  102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       "  'labels': TensorCategory([0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersLMBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    # @delegates\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False, \n",
    "                 mlm_probability=0.15, **kwargs):\n",
    "        tok_tfm = TokTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls, \n",
    "                 config=config, tokenizer=tokenizer, return_special_tokens_mask=True, is_lm=True, **kwargs)\n",
    "        \n",
    "        batch_tfms = LMBatchTfm(pretrained_model_name, tokenizer_cls, config, tokenizer, \n",
    "                                mlm=mlm, masking_func=masking_func, whole_word_masking=whole_word_masking,\n",
    "                                mlm_probability=mlm_probability)\n",
    "        create_batch = compose(untuple, DataCollatorForLanguageModeling(tokenizer), to_tuple)\n",
    "        return super().__init__(dl_type=TfmdDL,\n",
    "                                type_tfms=tok_tfm,\n",
    "                                batch_tfms=batch_tfms,\n",
    "                                dls_kwargs={'create_batch': fa_convert},\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], return_attention_mask=True, return_special_tokens_mask=True, verbose=False)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-baca2dc48733f0f6\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-baca2dc48733f0f6/0.0.0)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-baca2dc48733f0f6/0.0.0/cache-032c7a403e6a8cc8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-baca2dc48733f0f6/0.0.0/cache-bd4c0fdd3d96b02e.arrow\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_length = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "ds = datasets.Dataset.from_csv((path/'texts.csv').as_posix())\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "block_size = max_length\n",
    "lm_ds = ds.map(group_texts, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks=[TransformersLMBlock(tokenizer=tokenizer)],\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(lm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actually kinda funny. after awhile the pool shot feels like a [MASK] channel [MASK] s station identification logo, reminding us that we are watching \" grey matter \". &lt; br / &gt; &lt; br / &gt; i also enjoyed [MASK] boutshore name - calling. at one [MASK] an angry test subject taunts somebody in charge by [MASK] her a \" scientific b * tch! \". it'ambrose just a [MASK] inadequate [MASK]. several scenes later a different subject lets off steam by muttering about that \" scientific b * * tard! \". it just sounded very awkward to me. &lt; br / &gt; &lt; br / &gt; someday this movie [MASK] disappear forever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: the [MASK] men [MASK] all hotter than the [MASK] guys. &lt; br [MASK] &gt; &lt; br / [MASK] don [MASK] t get me wrong, im not saying all the gay guys were ugly and [MASK], [MASK] [MASK] [MASK] of fact i found some of them [MASK] cute. it's just that overall they were just blah compared to the men you [MASK] purse see on shows like avocation at love with [MASK]a tequila or [MASK] bachelor [MASK]. &lt; br / [MASK] [MASK] br / &gt; i don't know how many times i hit fast [MASK] during this show. i can accept a lead character as interesting as a cardboard [MASK], i can accept the mundane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i 1789 ever experienced. [MASK] possible line in the movie [MASK] unoriginal [MASK] cliche, or just [MASK] stupid. for [MASK] the name of the camp is \" camp blood \" ( lame ), the name of the clown is \" the [MASK] clown \" ( lame ). what is a clown doing in a forest anyway? was that [MASK] only mask they could find? 3. the last but certainly the least was the acting [MASK] [MASK] the worst [MASK] of actors and actresses ever assembled. a [MASK] cornucopia of shitty lines and poor acting. worst part by far was when [MASK] [MASK] flash back to this fat foreign girl getting naked for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s daughter )! as in the later film, too, fanfan ( [MASK] ideally cast gerard philipe [MASK], ironically, is so full of life here that one [MASK] it [MASK] [MASK] believe that he would be stricken down by cancer within 7 years'time ) [MASK] flanked by two fun - [MASK] yet [MASK]ly men ( one of them is actually his superior [MASK] and [MASK] heroine's own father ) and opposed [MASK] an unscrupulous figure within [MASK] own ranks ( the [MASK] [MASK] ro [MASK]vert, with whom the hero [MASK] engages in a rooftop duel since [MASK] too has amorous designs [MASK] the gypsy girl )! ; for the record</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(lm_ds, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]), 'input_ids': tensor([[ 2015,  2021,  2002,  ...,  2028, 16278,  2129],\n",
       "         [ 2685,  2024,  1037,  ...,  1037,  4393,  1010],\n",
       "         [ 2020,  2525,  5936,  ...,  3036,  6481,  1997],\n",
       "         ...,\n",
       "         [ 4516,  8223, 23242,  ..., 17529, 10652,  5092],\n",
       "         [ 2065,  2023,   103,  ...,   103,  2478,  7658],\n",
       "         [ 2369,  1996,  2616,  ...,  2166,  1010,  1998]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [ -100,  -100,  -100,  ...,  -100, 12954,  -100],\n",
       "         [ -100,  -100,  2276,  ...,  2003,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  -100]])},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_learner.ipynb.\n",
      "Converted 10_examples.classification-imdb.ipynb.\n",
      "Converted 11_examples.mlm-imdb.ipynb.\n",
      "Converted 12_examples.glue-benchmark.ipynb.\n",
      "Converted 12a_examples.glue-benchmark-sweeps.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

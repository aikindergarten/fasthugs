{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import Transform, ItemTransform\n",
    "from fastai.text.all import *\n",
    "from functools import partial\n",
    "from collections import UserString\n",
    "from transformers import (AutoTokenizer, AutoConfig, BatchEncoding,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask)\n",
    "\n",
    "from typing import Iterable, Union, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Transforms and DataBlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_splits(dataset, train='train', valid='validation'):\n",
    "    nt, nv = len(dataset[train]), len(dataset[valid])\n",
    "    return L(range(nt)), L(range(nt, nt+nv))\n",
    "\n",
    "# def DatasetSplitter(train='train', valid='validation'):\n",
    "#     return partial(get_splits, train=train, valid=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TitledStrEx(UserString):\n",
    "    \"TitledStr with option to set label\"\n",
    "    _show_args = {'label':'text'}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        label = kwargs.pop('label', None)\n",
    "        if label is not None:\n",
    "            self._show_args = {'label':label}\n",
    "        super().__init__(*args, **kwargs)\n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._show_args['label']\n",
    "    def truncate(self, n):\n",
    "        \"Truncate self to `n`\"\n",
    "        words = self.split(' ')[:n]\n",
    "        return TitledStrEx(' '.join(words), label=self.label)\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "text = TitledStrEx('here some words for you, boy', label='words')\n",
    "assert text == 'here some words for you, boy'\n",
    "assert text.truncate(3) == 'here some words'\n",
    "assert text.label == 'words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PreprocCategorize(DisplayedTransform):\n",
    "    \"Tranfrom for proper displaying preprocessed categorical labels\"\n",
    "    loss_func,order=CrossEntropyLossFlat(),1\n",
    "    def __init__(self, vocab=None):\n",
    "        store_attr()\n",
    "        \n",
    "    def encodes(self, o): return TensorCategory(o)\n",
    "    \n",
    "    def decodes(self, o): \n",
    "        if self.vocab is not None: return Category(self.vocab[o])\n",
    "        else: return Category(str(o.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def PreprocCategoryBlock(vocab:Union[List, NoneType]=None):\n",
    "    \"TransformBlock for preprocessed categorical labels with optional label names `vocab`\"\n",
    "    return TransformBlock(type_tfms=PreprocCategorize(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextGetter(ItemTransform):\n",
    "    \"Retrieves text fields `s1` and [optionally] `s2`. Adds corresponding prefixes\"\n",
    "    def __init__(self, s1:str='text', s2:str=None, prefix1:str='', prefix2:str=''):\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, sample):\n",
    "        if self.s2 is None: return self.prefix1 + sample[self.s1]\n",
    "        else: return self.prefix1+sample[self.s1], self.prefix2+sample[self.s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class KeyGetter(ItemTransform):\n",
    "    \"Returns a dict with `keys` retrieved from input sample\"\n",
    "    def __init__(self, keys:Iterable):\n",
    "        self.keys = set(keys)\n",
    "    \n",
    "    def encodes(self, sample):\n",
    "        # TODO warn when key is not in sample.keys()\n",
    "        return {k:v for k,v in sample.items() if k in self.keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransTensorText(TensorText): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:TransTensorText, y, samples, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    if isinstance(samples[0][0], tuple):\n",
    "        samples = L((*s[0], *s[1:]) for s in samples)\n",
    "        if trunc_at is not None: samples = L((s[0].truncate(trunc_at), s[1].truncate(trunc_at), *s[2:]) for s in samples)\n",
    "    if trunc_at is not None: samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    display_df(pd.DataFrame(ctxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_first(t, e):\n",
    "    for i, v in enumerate(t):\n",
    "        if v == e: return i\n",
    "        \n",
    "def split_by_sep(t, sep_tok_id):\n",
    "    idx = find_first(t, sep_tok_id)\n",
    "    return t[:idx], t[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokTransform(Transform):\n",
    "    \"Tokenizes single piece of text using pretrained tokenizer\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, is_lm=False,\n",
    "                 padding=False, truncation=False, max_length=None,\n",
    "                 preprocessed=False, skip_special_tokens=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, x):\n",
    "        if self.preprocessed:\n",
    "            toks = x\n",
    "        else:\n",
    "            toks = self.tokenizer(x,\n",
    "                          add_special_tokens=True,\n",
    "                          padding=self.padding,\n",
    "                          truncation=self.truncation,\n",
    "                          max_length=self.max_length,\n",
    "                          return_tensors='pt',\n",
    "                          **self.kwargs)\n",
    "        return toks\n",
    "\n",
    "    def decodes(self, x:TransTensorText):\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=self.skip_special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokBatchTransform(Transform):\n",
    "    \"\"\"\n",
    "    Tokenizes texts in batches using pretrained HuggingFace tokenizer.\n",
    "    The first element in a batch can be single string or 2-tuple of strings.\n",
    "    If `with_labels=True` the \"labels\" are added to the output dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None, \n",
    "                 do_targets=False, target_pad_id=-100, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # batch is a list of tuples of ({text or (text1, text2)}, {targets...})\n",
    "        if is_listy(batch[0][0]): # 1st element is tuple\n",
    "            self._two_texts = True\n",
    "            texts = ([s[0][0] for s in batch], [s[0][1] for s in batch])\n",
    "        elif is_listy(batch[0]): \n",
    "            texts = ([s[0] for s in batch],)\n",
    "        else: # batch is list of texts\n",
    "            texts = (list(batch),)\n",
    "            batch = [(s, ) for s in batch]\n",
    "        inps = self.tokenizer(*texts,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        \n",
    "        if self.do_targets and isinstance(batch[0][1], str):\n",
    "            target_texts = [s[1] for s in batch]\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                target_enc = self.tokenizer(target_texts,\n",
    "                                  padding=self.padding,\n",
    "                                  truncation=self.truncation,\n",
    "                                  max_length=self.max_length,\n",
    "                                  return_tensors='pt', \n",
    "                                  **self.kwargs)\n",
    "            targets = target_enc.input_ids\n",
    "            if self.target_pad_id != self.tokenizer.pad_token_id:\n",
    "                tgt_attn_mask = target_enc.attention_mask.to(torch.bool)\n",
    "                targets = torch.where(tgt_attn_mask, targets, -100)\n",
    "            targets = (TransTensorText(targets), )\n",
    "        else:\n",
    "            # inps are batched, collate targets into batches too\n",
    "            targets = default_collate([s[1:] for s in batch])\n",
    "        if self.with_labels:\n",
    "            # TODO consider cases when there are multiple labels\n",
    "            inps['labels'] = targets[0]\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            res = (inps, ) + tuple(targets)\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TransTensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        if self.do_targets:\n",
    "            x = torch.where(x == -100, self.tokenizer.pad_token_id, x)\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PadBatchTransform(Transform):\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None,\n",
    "                 do_targets=False, target_pad_id=-100, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        toks = [s[0] for s in samples]\n",
    "        if self.do_targets and ('labels' in toks[0].keys()):\n",
    "            label_lens = [len(s['labels']) for s in toks]\n",
    "            max_label_length = max(label_lens)\n",
    "            padding_side = self.tokenizer.padding_side\n",
    "            for tok, label_len in zip(toks, label_lens):\n",
    "                remainder = [self.target_pad_id] * (max_label_length - label_len)\n",
    "                tok[\"labels\"] = (tok[\"labels\"] + remainder\n",
    "                                 if padding_side==\"right\" else\n",
    "                                 remainder + tok[\"labels\"])\n",
    "        inps = self.tokenizer.pad(toks,\n",
    "                              padding=self.padding,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        inps = {k:TransTensorText(v) for k, v in inps.items() if (isinstance(v, torch.Tensor) and v.dim()>1)}\n",
    "        # inps are batched, collate targets into batches too\n",
    "        labels = default_collate([s[1:] for s in samples])\n",
    "        res = (inps, ) + tuple(labels)\n",
    "        return res\n",
    "\n",
    "    def decodes(self, x:TransTensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        if self.do_targets:\n",
    "            x = torch.where(x == -100, self.tokenizer.pad_token_id, x)\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def untuple(l):\n",
    "    return [e[0] for e in l]\n",
    "\n",
    "def to_tuple(x):\n",
    "    return (x, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- verify CLM works as well and mb rename `masking_func` as it would be not only for masking\n",
    "- add permutation LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LMBatchTfm(Transform):\n",
    "    \"Collates batch of pretokenized and chunked inputs into a batch and creates labels as defined by `masking_func`\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False,\n",
    "                 mlm_probability=0.15):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        if masking_func is None:\n",
    "            masking_func = (DataCollatorForWholeWordMask(tokenizer, mlm, mlm_probability)\n",
    "                            if whole_word_masking else\n",
    "                            DataCollatorForLanguageModeling(tokenizer, mlm, mlm_probability))\n",
    "        self.masking_func = masking_func\n",
    "        self.batch_processor = compose(untuple, masking_func, to_tuple)\n",
    "\n",
    "    def encodes(self, b):\n",
    "        # we get list of tuples but need a list of dicts\n",
    "        return self.batch_processor(b)\n",
    "\n",
    "    def decodes(self, b:(dict, BatchEncoding)):\n",
    "        if 'input_ids' in b: res = TransTensorText(b['input_ids'])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Undict(ItemTransform):\n",
    "    \n",
    "    def decodes(self, b):\n",
    "        # this is done hacky way to make show_batch work both when labels are separate and when in dict\n",
    "        # should be a better way\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TransTensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (x['labels'], )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class UndictS2S(ItemTransform):\n",
    "\n",
    "    def decodes(self, b):\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TransTensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (TransTensorText(x['labels']), )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(TokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, preprocessed=False, do_targets=False, \n",
    "                 group_by_len=True, **kwargs):\n",
    "        batch_tfm_cls = PadBatchTransform if preprocessed else TokBatchTransform\n",
    "        before_batch_tfm = batch_tfm_cls(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls,\n",
    "                 config=config, tokenizer=tokenizer, do_targets=do_targets, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL if group_by_len else TfmdDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=UndictS2S() if do_targets else Undict()\n",
    "                               )\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_tokenizer(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, ):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cd806fc98740a3858d858e110b698c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca7a12c883b43dcaaca74c116c34320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e88130c45744e891930787eaf0fb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6369d726d7455d81d3f2ffe816dd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "texts = pd.read_csv(path/'texts.csv')\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_len = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer),\n",
    "                             CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the shop around the corner is one of the sweetest and most feel - good romantic comedies ever made. there's just no getting around that, and it's hard to actually put one's feeling for this film into words. it's not one of those films that tries too hard, nor does it come up with the oddest possible scenarios to get the two protagonists together in the end. in fact, all its charm is innate, contained within the characters and the setting and the plot... which is highly believable to boot. it's easy to think that such a love story, as beautiful as any other ever told, * could * happen to you... a feeling you don't often get from other romantic comedies, however sweet and heart - warming they may be. &lt; br / &gt; &lt; br / &gt; alfred kralik ( james stewart ) and clara novak ( margaret</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>now that che ( 2008 ) has finished its relatively short australian cinema run ( extremely limited release : 1 screen in sydney, after 6wks ), i can guiltlessly join both hosts of \" at the movies \" in taking steven soderbergh to task. &lt; br / &gt; &lt; br / &gt; it's usually satisfying to watch a film director change his style / subject, but soderbergh's most recent stinker, the girlfriend experience ( 2009 ), was also missing a story, so narrative ( and editing? ) seem to suddenly be soderbergh's main challenge. strange, after 20 - odd years in the business. he was probably never much good at narrative, just hid it well inside \" edgy \" projects. &lt; br / &gt; &lt; br / &gt; none of this excuses him this present, almost diabolical failure. as david stratton warns, \" two parts of che don't ( even</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this film sat on my tivo for weeks before i watched it. i dreaded a self - indulgent yuppie flick about relationships gone bad. i was wrong ; this was an engrossing excursion into the screwed - up libidos of new yorkers. &lt; br / &gt; &lt; br / &gt; the format is the same as max ophuls'\" la ronde, \" based on a play by arthur schnitzler, who is given an \" inspired by \" credit. it starts from one person, a prostitute, standing on a street corner in brooklyn. she is picked up by a home contractor, who has sex with her on the hood of a car, but can't come. he refuses to pay her. when he's off peeing, she answers his cell phone and takes a message. she runs away with his keys. &lt; br / &gt; &lt; br / &gt; then the story switches to</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(texts, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  1996,  4497,  ...,  2090,  1005,   102],\n",
       "          [  101,  2116, 19046,  ...,  1010,  2004,   102],\n",
       "          ...,\n",
       "          [  101,  2024,  2017,  ...,  2015,  2000,   102],\n",
       "          [  101,  1045,  3427,  ...,  2091,  1012,   102],\n",
       "          [  101,  2348,  3858,  ...,  1997,  1996,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')},\n",
       " TensorCategory([0, 1, 1, 0, 0, 0, 0, 1], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace models can compute loss, to use loss computed by model you should pass `with_labels = True` to datablock constructor. The `show_batch` result didn't change, but actually the labels are moved to `dict` object, which is the first element of a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the shop around the corner is one of the sweetest and most feel - good romantic comedies ever made. there's just no getting around that, and it's hard to actually put one's feeling for this film into words. it's not one of those films that tries too hard, nor does it come up with the oddest possible scenarios to get the two protagonists together in the end. in fact, all its charm is innate, contained within the characters and the setting and the plot... which is highly believable to boot. it's easy to think that such a love story, as beautiful as any other ever told, * could * happen to you... a feeling you don't often get from other romantic comedies, however sweet and heart - warming they may be. &lt; br / &gt; &lt; br / &gt; alfred kralik ( james stewart ) and clara novak ( margaret</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well, what can i say. &lt; br / &gt; &lt; br / &gt; \" what the bleep do we know \" has achieved the nearly impossible - leaving behind such masterpieces of the genre as \" the postman \", \" the dungeon master \", \" merlin \", and so fourth, it will go down in history as the single worst movie i have ever seen in its entirety. and that, ladies and gentlemen, is impressive indeed, for i have seen many a bad movie. &lt; br / &gt; &lt; br / &gt; this masterpiece of modern cinema consists of two interwoven parts, alternating between a silly and contrived plot about an extremely annoying photographer, abandoned by her husband and forced to take anti - depressants to survive, and a bunch of talking heads going on about how quantum physics supposedly justifies their new - agy pseudo - philosophy. basically, if</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the year 2005 saw no fewer than 3 filmed productions of h. g. wells'great novel, \" war of the worlds \". this is perhaps the least well - known and very probably the best of them. no other version of wotw has ever attempted not only to present the story very much as wells wrote it, but also to create the atmosphere of the time in which it was supposed to take place : the last year of the 19th century, 1900 using wells'original setting, in and near woking, england. &lt; br / &gt; &lt; br / &gt; imdb seems unfriendly to what they regard as \" spoilers \". that might apply with some films, where the ending might actually be a surprise, but with regard to one of the most famous novels in the world, it seems positively silly. i have no sympathy for people who have neglected to</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer, with_labels=True), CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())\n",
    "dls = dblock.dataloaders(texts, bs=8)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 101, 6274, 5125,  ..., 1998, 2130,  102],\n",
       "          [ 101, 1996, 4497,  ..., 2090, 1005,  102],\n",
       "          [ 101, 2085, 2008,  ..., 2839, 1025,  102],\n",
       "          ...,\n",
       "          [ 101, 2024, 2017,  ..., 2015, 2000,  102],\n",
       "          [ 101, 1045, 2453,  ..., 1997, 2079,  102],\n",
       "          [ 101, 2348, 3858,  ..., 1997, 1996,  102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       "  'labels': TensorCategory([0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersLMBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for language modelling using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(TokTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False,\n",
    "                 mlm_probability=0.15, preprocessed=True, group_by_len=False, **kwargs):\n",
    "        tok_tfm = TokTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls,\n",
    "                               config=config, tokenizer=tokenizer, return_special_tokens_mask=True, is_lm=True,\n",
    "                               preprocessed=preprocessed, **kwargs)\n",
    "\n",
    "        batch_tfms = LMBatchTfm(pretrained_model_name, tokenizer_cls, config, tokenizer,\n",
    "                                mlm=mlm, masking_func=masking_func, whole_word_masking=whole_word_masking,\n",
    "                                mlm_probability=mlm_probability)\n",
    "        \n",
    "        return super().__init__(dl_type=SortedDL if group_by_len else TfmdDL,\n",
    "                                type_tfms=tok_tfm,\n",
    "                                batch_tfms=batch_tfms,\n",
    "                                dls_kwargs={'create_batch': fa_convert})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], return_attention_mask=True, return_special_tokens_mask=True, verbose=False)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f78ec54769bd79c4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-f78ec54769bd79c4/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe4327abc6c4d688a5b97a4fb5ac885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f78ec54769bd79c4/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3f3671197c451885ef5d775cb16251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850f27be45324155826dff0dcb75a6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_length = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "ds = datasets.Dataset.from_csv((path/'texts.csv').as_posix())\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "block_size = max_length\n",
    "lm_ds = ds.map(group_texts, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks=[TransformersLMBlock(tokenizer=tokenizer)],\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(lm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get any better because [MASK] plot is flawed [MASK] begin with [MASK] it never works [MASK] and like [MASK] predecessors, [MASK] acting is med [MASK]cre. &lt; treat / [MASK] &lt; br [MASK] &gt; [MASK] plot has a [MASK] ending which will surprise any one who has never seen [MASK] movie before [MASK] the ending doesn't [MASK] the story. [MASK] this movie ended [MASK] minutes earlier, it would have worked and have been very satisfying and i [MASK] have thought it more worthwhile [MASK] but here is the spoiler and that [MASK] the end crime does pay because the criminal 42 not caught. i never like this message resulting from a movie. [SEP] [CLS] warning! [MASK] review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girls and i think i was really kind giving it a 4 out of 10 kn what could [MASK] hart been a wonderful story [MASK] actually talked set of more or less decent actors became a total farce in my eyes. there are so [MASK] [MASK] [MASK]s in that flick, the women'[MASK] [MASK] is just awful [MASK] most of the scenes are more than unrealistic or seem fake andhra there's no real passion in this movie but a bunch [MASK] actors over [MASK] acting over any limits that it hurts. it's boise funny enough to be a [MASK], it's too [MASK] - sad to really touch, so in my eyes it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>useless and the direction [MASK] quite unoriginal when it comes to [MASK]ogs scenes. but all that [MASK]'t really matter, for [MASK] the bourne ultimatum [MASK] is an action [MASK]. and the action scenes are rather impressive. [MASK] br / &gt; &lt; br / &gt; everyone here is talking [MASK] the \" waterloo scene \" and the \" tanger pursuit \" and everyone [MASK] s right. i [MASK] enjoyed the fight in tanger, that reminds my [MASK] its exaggeration and crazi [MASK] the works of tsui hark. visually inventive scenes, [MASK] of intelligent [MASK] parts and a good reflection on [MASK]'s contemporary [MASK]s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>close [MASK] is released from prison after being \" cured \" of her obsession with fur by a psychologist named dr. pavlov ( ugh! ) [MASK] but the \" cure \" is broken when cruella hears the toll of [MASK] ben, and she once again goes on a mad [MASK] to make herself the perfect coat out of dalmation [MASK]. &lt; br / &gt; &lt; br / &gt; this movie [MASK] bad on so [MASK] levels, starting with the [MASK] that it'[MASK] a \" thanksgiving [MASK] schlock \" movie designed to suck every last available dime out of the disney [MASK] machine [MASK] glenn [MASK] over - over - over - over - acts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(lm_ds, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]), 'input_ids': tensor([[1997, 1037, 2321,  ..., 1028,  103, 7987],\n",
       "         [3132,  103, 1997,  ..., 1010,  103,  103],\n",
       "         [2323, 2191, 2178,  ..., 5574, 2000,  103],\n",
       "         ...,\n",
       "         [1038, 1007, 1996,  ..., 1013, 1028, 1996],\n",
       "         [ 103, 2000, 3579,  ..., 1051, 1012, 1048],\n",
       "         [2065, 3087, 4282,  ..., 8213, 2015, 1998]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  1026,  -100],\n",
       "         [ -100,  2846,  -100,  ...,  -100, 27118,  6491],\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  2033],\n",
       "         ...,\n",
       "         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ 3754,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  ..., 15667,  -100,  -100]])},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiChoiceTransform(Transform):\n",
    "    \"\"\"\n",
    "    Processes inputs for multiple choice\n",
    "    \"\"\"\n",
    "    def __init__(self, sentence_keys, ending_keys,\n",
    "                 pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, with_labels=False, padding=True, \n",
    "                 truncation=True, max_length=None, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # inputs are list of tuple(dict, label)\n",
    "        inps = [b[0] for b in batch]\n",
    "        sk1, sk2 = self.sentence_keys\n",
    "        num_endings = len(self.ending_keys)\n",
    "        texts1, texts2 = [], []\n",
    "        for s in inps:\n",
    "            texts1.extend([s[sk1]]*num_endings)\n",
    "            texts2.extend([f\"{s[sk2]} {s[e]}\" for e in self.ending_keys])\n",
    "        inps = self.tokenizer(texts1, texts2,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        inps = {k:v.reshape(-1, num_endings, v.size(1)) for k,v in inps.items()}\n",
    "        \n",
    "        targets = default_collate([s[1:] for s in batch])\n",
    "        if self.with_labels:\n",
    "            inps['labels'] = targets[0]\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            res = (inps, ) + tuple(targets)\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TransTensorText):\n",
    "        endings = ()\n",
    "        for i, l in enumerate(self.ending_keys):\n",
    "            x1, x2 = split_by_sep(x[i, :], self.tokenizer.sep_token_id)\n",
    "            endings += (TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True), label=l),)\n",
    "        return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True), label=self.sentence_keys[0]), ) + endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiChoiceBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for multiple choice using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(MultiChoiceTransform)\n",
    "    def __init__(self, sentence_keys, ending_keys, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, preprocessed=False, group_by_len=True,\n",
    "                 **kwargs):\n",
    "        batch_tfm_cls = MultiChoiceTransform\n",
    "        before_batch_tfm = batch_tfm_cls(sentence_keys, ending_keys, pretrained_model_name=pretrained_model_name, \n",
    "                tokenizer_cls=tokenizer_cls, config=config, tokenizer=tokenizer, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL if group_by_len else TfmdDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=Undict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PadTokBatchTransform(Transform):\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, with_labels=False, padding=True, \n",
    "                 truncation=True, max_length=None, label_vocab=None,\n",
    "                 target_pad_id=-100, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        toks = [s[0] for s in samples]\n",
    "        # the labels are expected to be found either in dictionary with tokens\n",
    "        # or as element 1 of each sample\n",
    "        labels = ([s['labels'] for s in toks] \n",
    "                    if ('labels' in toks[0].keys()) else\n",
    "                    [s[1] for s in samples])\n",
    "\n",
    "        label_lens = [len(l) for l in labels]\n",
    "        max_label_length = max(label_lens)\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        for tok, label, label_len in zip(toks, labels, label_lens):\n",
    "            remainder = [self.target_pad_id] * (max_label_length - label_len)\n",
    "            tok[\"labels\"] = (label + remainder\n",
    "                             if padding_side==\"right\" else\n",
    "                             remainder + label)\n",
    "        inps = self.tokenizer.pad(toks,\n",
    "                              padding=self.padding,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        labels = inps.pop('labels')\n",
    "        inps = {k:TransTensorText(v) for k, v in inps.items() if (isinstance(v, torch.Tensor) and v.dim()>1)}\n",
    "        if self.with_labels:\n",
    "            inps['labels'] = labels\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            res = (inps, ) + (labels, )\n",
    "        return res\n",
    "\n",
    "    def decodes(self, x:TransTensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))\n",
    "    def decodes(self, x):\n",
    "        if self.label_vocab is not None:\n",
    "            res = [self.label_vocab[e] for e in x if e != -100]\n",
    "        else:\n",
    "            res = [e for e in x if e != -100]\n",
    "        return TitledStrEx(''.join(f'{x}, ' for x in res), label='tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenClassificationBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for token classification using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(PadTokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, with_labels=True, label_vocab=None, \n",
    "                 group_by_len=True, **kwargs):\n",
    "        before_batch_tfm = PadTokBatchTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls,\n",
    "                 config=config, tokenizer=tokenizer, label_vocab=label_vocab, with_labels=True, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL if group_by_len else TfmdDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=Undict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_learner.ipynb.\n",
      "Converted 02_metrics.ipynb.\n",
      "Converted 10_examples.classification-imdb.ipynb.\n",
      "Converted 11_examples.mlm-imdb.ipynb.\n",
      "Converted 12_examples.glue-benchmark.ipynb.\n",
      "Converted 12a_examples.glue-benchmark-sweeps.ipynb.\n",
      "Converted 14_examples.machine_translation.ipynb.\n",
      "Converted 15_examples.summarization.ipynb.\n",
      "Converted 16_examples.multiple_choice.ipynb.\n",
      "Converted 17_examples.token_classification.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import Transform, ItemTransform\n",
    "from fastai.text.all import *\n",
    "from functools import partial\n",
    "from collections import UserString\n",
    "from transformers import (AutoTokenizer, AutoConfig, BatchEncoding,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask)\n",
    "\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Transforms and DataBlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_splits(dataset, train='train', valid='validation'):\n",
    "    nt, nv = len(dataset[train]), len(dataset[valid])\n",
    "    return L(range(nt)), L(range(nt, nt+nv))\n",
    "\n",
    "# def DatasetSplitter(train='train', valid='validation'):\n",
    "#     return partial(get_splits, train=train, valid=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TitledStrEx(UserString):\n",
    "    \"TitledStr with option to set label\"\n",
    "    _show_args = {'label':'text'}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        label = kwargs.pop('label', None)\n",
    "        if label is not None:\n",
    "            self._show_args = {'label':label}\n",
    "        super().__init__(*args, **kwargs)\n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._show_args['label']\n",
    "    def truncate(self, n):\n",
    "        \"Truncate self to `n`\"\n",
    "        words = self.split(' ')[:n]\n",
    "        return TitledStrEx(' '.join(words), label=self.label)\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "text = TitledStrEx('here some words for you, boy', label='words')\n",
    "assert text == 'here some words for you, boy'\n",
    "assert text.truncate(3) == 'here some words'\n",
    "assert text.label == 'words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextGetter(ItemTransform):\n",
    "    \"Retrieves text fields `s1` and [optionally] `s2`. Adds corresponding prefixes\"\n",
    "    def __init__(self, s1:str='text', s2:str=None, prefix1:str='', prefix2:str=''):\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, sample):\n",
    "        if self.s2 is None: return self.prefix1 + sample[self.s1]\n",
    "        else: return self.prefix1+sample[self.s1], self.prefix2+sample[self.s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class KeyGetter(ItemTransform):\n",
    "    \"Returns a dict with `keys` retrieved from input sample\"\n",
    "    def __init__(self, keys:Iterable):\n",
    "        self.keys = set(keys)\n",
    "    \n",
    "    def encodes(self, sample):\n",
    "        # TODO warn when key is not in sample.keys()\n",
    "        return {k:v for k,v in sample.items() if k in self.keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransTensorText(TensorText): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:TransTensorText, y, samples, ctxs=None, max_n=10, trunc_at=150, **kwargs):\n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    if isinstance(samples[0][0], tuple):\n",
    "        samples = L((*s[0], *s[1:]) for s in samples)\n",
    "        if trunc_at is not None: samples = L((s[0].truncate(trunc_at), s[1].truncate(trunc_at), *s[2:]) for s in samples)\n",
    "    if trunc_at is not None: samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    display_df(pd.DataFrame(ctxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_first(t, e):\n",
    "    for i, v in enumerate(t):\n",
    "        if v == e: return i\n",
    "        \n",
    "def split_by_sep(t, sep_tok_id):\n",
    "    idx = find_first(t, sep_tok_id)\n",
    "    return t[:idx], t[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokTransform(Transform):\n",
    "    \"Tokenizes single piece of text using pretrained tokenizer\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False,\n",
    "                 padding=False, truncation=False, max_length=None, \n",
    "                 preprocessed=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "        \n",
    "    def encodes(self, x):\n",
    "        if self.preprocessed:\n",
    "            toks = x\n",
    "        else:\n",
    "            toks = self.tokenizer(x,\n",
    "                          add_special_tokens=True,\n",
    "                          padding=self.padding,\n",
    "                          truncation=self.truncation,\n",
    "                          max_length=self.max_length,\n",
    "                          return_tensors='pt',\n",
    "                          **self.kwargs)\n",
    "        return toks\n",
    "    \n",
    "    def decodes(self, x:TransTensorText):\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokBatchTransform(Transform):\n",
    "    \"\"\"\n",
    "    Tokenizes texts in batches using pretrained HuggingFace tokenizer.\n",
    "    The first element in a batch can be single string or 2-tuple of strings.\n",
    "    If `with_labels=True` the \"labels\" are added to the output dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None, \n",
    "                 do_targets=False, target_pad_id=-100, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # batch is a list of tuples of ({text or (text1, text2)}, {targets...})\n",
    "        if is_listy(batch[0][0]): # 1st element is tuple\n",
    "            self._two_texts = True\n",
    "            texts = ([s[0][0] for s in batch], [s[0][1] for s in batch])\n",
    "        elif is_listy(batch[0]): \n",
    "            texts = ([s[0] for s in batch],)\n",
    "        else: # batch is list of texts\n",
    "            texts = (list(batch),)\n",
    "            batch = [(s, ) for s in batch]\n",
    "        inps = self.tokenizer(*texts,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        \n",
    "        if self.do_targets and isinstance(batch[0][1], str):\n",
    "            target_texts = [s[1] for s in batch]\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                target_enc = self.tokenizer(target_texts,\n",
    "                                  padding=self.padding,\n",
    "                                  truncation=self.truncation,\n",
    "                                  max_length=self.max_length,\n",
    "                                  return_tensors='pt', \n",
    "                                  **self.kwargs)\n",
    "            targets = target_enc.input_ids\n",
    "            if self.target_pad_id != self.tokenizer.pad_token_id:\n",
    "                tgt_attn_mask = target_enc.attention_mask.to(torch.bool)\n",
    "                targets = torch.where(tgt_attn_mask, targets, -100)\n",
    "            targets = (TransTensorText(targets), )\n",
    "        else:\n",
    "            # inps are batched, collate targets into batches too\n",
    "            targets = default_collate([s[1:] for s in batch])\n",
    "        if self.with_labels:\n",
    "            # TODO consider cases when there are multiple labels\n",
    "            inps['labels'] = targets[0]\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            res = (inps, ) + tuple(targets)\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TransTensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        if self.do_targets:\n",
    "            x = torch.where(x == -100, self.tokenizer.pad_token_id, x)\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PadBatchTransform(Transform):\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None,\n",
    "                 do_targets=False, target_pad_id=-100, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        toks = [s[0] for s in samples]\n",
    "        if self.do_targets and ('labels' in toks[0].keys()):\n",
    "            label_lens = [len(s['labels']) for s in toks]\n",
    "            max_label_length = max(label_lens)\n",
    "            padding_side = self.tokenizer.padding_side\n",
    "            for tok, label_len in zip(toks, label_lens):\n",
    "                remainder = [self.target_pad_id] * (max_label_length - label_len)\n",
    "                tok[\"labels\"] = (tok[\"labels\"] + remainder\n",
    "                                 if padding_side==\"right\" else\n",
    "                                 remainder + tok[\"labels\"])\n",
    "        inps = self.tokenizer.pad(toks,\n",
    "                              padding=self.padding,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        inps = {k:TransTensorText(v) for k, v in inps.items() if (isinstance(v, torch.Tensor) and v.dim()>1)}\n",
    "        # inps are batched, collate targets into batches too\n",
    "        labels = default_collate([s[1:] for s in samples])\n",
    "        res = (inps, ) + tuple(labels)\n",
    "        return res\n",
    "\n",
    "    def decodes(self, x:TransTensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        if self.do_targets:\n",
    "            x = torch.where(x == -100, self.tokenizer.pad_token_id, x)\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def untuple(l):\n",
    "    return [e[0] for e in l]\n",
    "\n",
    "def to_tuple(x):\n",
    "    return (x, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- verify CLM works as well and mb rename `masking_func` as it would be not only for masking\n",
    "- add permutation LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LMBatchTfm(Transform):\n",
    "    \"Collates batch of pretokenized and chunked inputs into a batch and creates labels as defined by `masking_func`\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False,\n",
    "                 mlm_probability=0.15):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        if masking_func is None:\n",
    "            masking_func = (DataCollatorForWholeWordMask(tokenizer, mlm, mlm_probability) \n",
    "                            if whole_word_masking else \n",
    "                            DataCollatorForLanguageModeling(tokenizer, mlm, mlm_probability))\n",
    "        self.masking_func = masking_func\n",
    "        self.batch_processor = compose(untuple, masking_func, to_tuple)\n",
    "            \n",
    "    def encodes(self, b):\n",
    "        # we get list of tuples but need a list of dicts\n",
    "        return self.batch_processor(b)\n",
    "    \n",
    "    def decodes(self, b:(dict, BatchEncoding)):\n",
    "        if 'input_ids' in b: res = TransTensorText(b['input_ids'])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Undict(ItemTransform):\n",
    "    \n",
    "    def decodes(self, b):\n",
    "        # this is done hacky way to make show_batch work both when labels are separate and when in dict\n",
    "        # should be a better way\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TransTensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (x['labels'], )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class UndictS2S(ItemTransform):\n",
    "\n",
    "    def decodes(self, b):\n",
    "        x = b[0]\n",
    "        if 'input_ids' in x: res = (TransTensorText(x['input_ids']), )\n",
    "        if 'labels' in x: res += (TransTensorText(x['labels']), )\n",
    "        return res + tuple(b[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(TokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, preprocessed=False, do_targets=False, \n",
    "                 group_by_len=True, **kwargs):\n",
    "        batch_tfm_cls = PadBatchTransform if preprocessed else TokBatchTransform\n",
    "        before_batch_tfm = batch_tfm_cls(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls,\n",
    "                 config=config, tokenizer=tokenizer, do_targets=do_targets, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL if group_by_len else TfmdDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=UndictS2S() if do_targets else Undict()\n",
    "                               )\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_tokenizer(cls, ):\n",
    "#         pass\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, ):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "texts = pd.read_csv(path/'texts.csv')\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_len = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer),\n",
    "                             CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now that che ( 2008 ) has finished its relatively short australian cinema run ( extremely limited release : 1 screen in sydney, after 6wks ), i can guiltlessly join both hosts of \" at the movies \" in taking steven soderbergh to task. &lt; br / &gt; &lt; br / &gt; it's usually satisfying to watch a film director change his style / subject, but soderbergh's most recent stinker, the girlfriend experience ( 2009 ), was also missing a story, so narrative ( and editing? ) seem to suddenly be soderbergh's main challenge. strange, after 20 - odd years in the business. he was probably never much good at narrative, just hid it well inside \" edgy \" projects. &lt; br / &gt; &lt; br / &gt; none of this excuses him this present, almost diabolical failure. as david stratton warns, \" two parts of che don't ( even</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i really wanted to love this show. i truly, honestly did. &lt; br / &gt; &lt; br / &gt; for the first time, gay viewers get their own version of the \" the bachelor \". with the help of his obligatory \" hag \" andra, james, a good looking, well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ). the only problem is half of them are straight and james doesn't know this. if james picks a gay one, they get a trip to new zealand, and if he picks a straight one, straight guy gets $ 25, 000. how can this not be fun?! take my hand, lets stroll : &lt; br / &gt; &lt; br / &gt; the most glaring problem with this show is the bachelor himself.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(texts, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  1996,  4497,  ...,  2090,  1005,   102],\n",
       "          [  101,  1996,  1038,  ...,  6300, 10376,   102],\n",
       "          ...,\n",
       "          [  101,  1045,  2018,  ...,  1007,  1012,   102],\n",
       "          [  101,  2348,  3858,  ...,  1997,  1996,   102],\n",
       "          [  101,  2023,  2003,  ...,  2137,  1999,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')},\n",
       " TensorCategory([0, 1, 0, 0, 0, 0, 1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace models can compute loss, to use loss computed by model you should pass `with_labels = True` to datablock constructor. The `show_batch` result didn't change, but actually the labels are moved to `dict` object, which is the first element of a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic storyline would make the film critic proof. he was right, but it didn't fool me. raising victor vargas is the story about a seventeen - year old boy called, you guessed it, victor vargas ( victor rasuk ) who lives his teenage years chasing more skirt than the rolling stones could do</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many neglect that this isn't just a classic due to the fact that it's the first 3d game, or even the first shoot -'em - up. it's also one of the first stealth games, one of the only ( and definitely the first ) truly claustrophobic games, and just a pretty well - rounded gaming experience in general. with graphics that are terribly dated today, the game thrusts you into the role of b. j. ( don't even * think * i'm going to attempt spelling his last name! ), an american p. o. w. caught in an underground bunker. you fight and search your way through tunnels in order to achieve different objectives for the six episodes ( but, let's face it, most of them are just an excuse to hand you a weapon, surround you with nazis and send you out to waste one of the nazi leaders</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the blob starts with one of the most bizarre theme songs ever, sung by an uncredited burt bacharach of all people! you really have to hear it to believe it, the blob may be worth watching just for this song alone &amp; my user comment summary is just a little taste of the classy lyrics... after this unnerving opening credits sequence the blob introduces us, the viewer that is, to steve andrews ( steve mcqueen as steven mcqueen ) &amp; his girlfriend jane martin ( aneta corsaut ) who are parked on their own somewhere &amp; witness what looks like a meteorite falling to earth in nearby woods. an old man ( olin howland as olin howlin ) who lives in a cabin also sees it &amp; goes to investigate, he finds a crater &amp; a strange football sized rock which splits open when he unwisely pokes it with a</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i rented the dubbed - english version of lensman, hoping that since it came from well - known novels it would have some substance. while there were hints of substance in the movie, it mostly didn't rise above the level of kiddie cartoon. maybe the movie was a bad adaptation of the book, or it lost a lot in the dubbed version. or maybe even the source novels were lightweight. but for whatever reason, there wasn't much there. &lt; br / &gt; &lt; br / &gt; i noticed lots of details that were derivative, sloppy, poorly dramatized, or otherwise deficient. some examples : the opening scenes looked borrowed from the 2001 \" star gate \" scene and the star wars image of hyperspace. the robot on the harvester looked like an anthropomorphized \" r2 - d2 \". &lt; br / &gt; &lt; br / &gt; it starts out trying to</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks = [TransformersTextBlock(tokenizer=tokenizer, with_labels=True), CategoryBlock()],\n",
    "                   get_x=ItemGetter('text'),\n",
    "                   get_y=ItemGetter('label'),\n",
    "                   splitter=ColSplitter())\n",
    "dls = dblock.dataloaders(texts, bs=8)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  6274,  5125,  ...,  1998,  2130,   102],\n",
       "          [  101,  1996,  4497,  ...,  2090,  1005,   102],\n",
       "          [  101,  2085,  2008,  ...,  2839,  1025,   102],\n",
       "          ...,\n",
       "          [  101,  2023,  2143,  ...,  1037,  2265,   102],\n",
       "          [  101,  2092,  1010,  ..., 15345,  1012,   102],\n",
       "          [  101,  1996,  1038,  ...,  6300, 10376,   102]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       "  'labels': TensorCategory([0, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersLMBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for language modelling using pretrained tokenizers from Huggingface\"\n",
    "    # @delegates\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, mlm=True, masking_func=None, whole_word_masking=False, \n",
    "                 mlm_probability=0.15, preprocessed=True, **kwargs):\n",
    "        tok_tfm = TokTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls, \n",
    "                               config=config, tokenizer=tokenizer, return_special_tokens_mask=True, is_lm=True,\n",
    "                               preprocessed=preprocessed, **kwargs)\n",
    "        \n",
    "        batch_tfms = LMBatchTfm(pretrained_model_name, tokenizer_cls, config, tokenizer, \n",
    "                                mlm=mlm, masking_func=masking_func, whole_word_masking=whole_word_masking,\n",
    "                                mlm_probability=mlm_probability)\n",
    "        create_batch = compose(untuple, DataCollatorForLanguageModeling(tokenizer), to_tuple)\n",
    "        return super().__init__(dl_type=TfmdDL,\n",
    "                                type_tfms=tok_tfm,\n",
    "                                batch_tfms=batch_tfms,\n",
    "                                dls_kwargs={'create_batch': fa_convert},\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], return_attention_mask=True, return_special_tokens_mask=True, verbose=False)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fcd59879e8ec0cc2\n",
      "Reusing dataset csv (/home/arto/.cache/huggingface/datasets/csv/default-fcd59879e8ec0cc2/0.0.0)\n",
      "Loading cached processed dataset at /home/arto/.cache/huggingface/datasets/csv/default-fcd59879e8ec0cc2/0.0.0/cache-e48d62f22574817d.arrow\n",
      "Loading cached processed dataset at /home/arto/.cache/huggingface/datasets/csv/default-fcd59879e8ec0cc2/0.0.0/cache-f4a996ce554fd629.arrow\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_length = 128\n",
    "bs = 8\n",
    "val_bs = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "ds = datasets.Dataset.from_csv((path/'texts.csv').as_posix())\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "block_size = max_length\n",
    "lm_ds = ds.map(group_texts, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "dblock = DataBlock(blocks=[TransformersLMBlock(tokenizer=tokenizer)],\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#hide\n",
    "# dblock.summary(lm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as diego garcia is a british colony, much like mauritius, the nearby island [MASK] where [MASK] natives were exiled, used [MASK] be chronicle but the british [MASK] has ignored their pleas to return to their homeland, [MASK] the island is [MASK] [MASK] military base for the [MASK] states army, who have used it as a basis for the bombing of iraq and afghanistan. &lt; br / [MASK] &lt; br / &gt; as usual, pilger [MASK] s coverage is shocking, especially as he documents the treatment [MASK] the current impoverished [MASK] conditions of the surviving islanders [MASK] his interviews [MASK] round are excellent, and his cornering of [MASK] parliament representative where he uses the government's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>re a vonnegut fan or [MASK] completely insane, don't see it. please. [SEP] [CLS] three stooges - have [MASK], will travel - [MASK] this was the first feature length film [MASK] [MASK] the st [MASK]ges and it is pretty och. it makes the three stooges go around the world in a daze ( from 1963 ) [MASK] [MASK] a masterpiece. &lt; br / [MASK] &lt; br / &gt; the [MASK] [MASK] [MASK] [MASK] janitors at a [MASK] place. they climb into a rocket and [MASK] goes to venus. they [MASK] some stuff there including a talking unicorn they call \" [MASK] [MASK] \" which they bring back to earth with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they [MASK] d have to see this lame attempt at movie - [MASK] on their account [MASK] the [MASK] overall is bad enough to be funny, and that'[MASK] about the best [MASK] i can [MASK] for it. [SEP] [CLS] as horror [MASK] we all know that blind rentals are a crap - shoot. sometimes we [MASK] a real gem, [MASK] many times we find that [MASK] film we'[MASK] just spent our hard earned money on [MASK] [MASK] more than [MASK] putrid steamer made worse by [MASK] completely und [MASK]rved rave reviews [MASK] film fest awards listed on the box. such [MASK] the case with [MASK] across the eyes ( a title i'm sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>montages. those were easily the zee songs and best performances in the film. [MASK] the \" rise to [MASK] top \" portion of the film was the only [MASK] of [MASK] film that had a consistent point of [MASK] or any momentum. [MASK] remaining hour and 45 minutes was a formless, rambling mess that was [MASK] realistic nor fantastic enough [MASK] [MASK] interesting. [MASK] was also visually dull and included too many sound - alike [MASK]. [MASK] br / [MASK] &lt; [MASK] / &gt; condon didn [MASK] t try to turn any of the tunes into big show pieces as i'd expected they would. each number in the 2nd half was just one closeup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "dls = dblock.dataloaders(lm_ds, bs=bs, val_bs=val_bs)\n",
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]), 'input_ids': tensor([[ 2102,  1024,  3533,  ...,  2045,  2024,  2053],\n",
       "         [ 3377,   103,  1996,  ...,  2000,  3786,  3168],\n",
       "         [ 2070,  9414, 12817,  ...,  1037,  1000,  2200],\n",
       "         ...,\n",
       "         [ 1010,  2065,  2017,  ...,  5691,  2000,  2191],\n",
       "         [ 2681,  1996,  2181,  ...,  4288,   103,  2015],\n",
       "         [ 2022,   103,  3294,  ...,  7275,  9409,  3122]]), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, 2053],\n",
       "         [-100, 1997, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, 5405, -100],\n",
       "         [-100, 1037, -100,  ..., -100, -100, -100]])},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiChoiceTransform(Transform):\n",
    "    \"\"\"\n",
    "    Processes inputs for multiple choice\n",
    "    \"\"\"\n",
    "    def __init__(self, sentence_keys, ending_keys,\n",
    "                 pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, with_labels=False, padding=True, \n",
    "                 truncation=True, max_length=None, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # inputs are list of tuple(dict, label)\n",
    "        inps = [b[0] for b in batch]\n",
    "        sk1, sk2 = self.sentence_keys\n",
    "        num_endings = len(self.ending_keys)\n",
    "        texts1, texts2 = [], []\n",
    "        for s in inps:\n",
    "            texts1.extend([s[sk1]]*num_endings)\n",
    "            texts2.extend([f\"{s[sk2]} {s[e]}\" for e in self.ending_keys])\n",
    "        inps = self.tokenizer(texts1, texts2,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        inps = {k:v.reshape(-1, num_endings, v.size(1)) for k,v in inps.items()}\n",
    "        \n",
    "        targets = default_collate([s[1:] for s in batch])\n",
    "        if self.with_labels:\n",
    "            inps['labels'] = targets[0]\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            res = (inps, ) + tuple(targets)\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TransTensorText):\n",
    "        endings = ()\n",
    "        for i, l in enumerate(self.ending_keys):\n",
    "            x1, x2 = split_by_sep(x[i, :], self.tokenizer.sep_token_id)\n",
    "            endings += (TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True), label=l),)\n",
    "        return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True), label=self.sentence_keys[0]), ) + endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiChoiceBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for multiple choice using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(MultiChoiceTransform)\n",
    "    def __init__(self, sentence_keys, ending_keys, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, preprocessed=False, group_by_len=True,\n",
    "                 **kwargs):\n",
    "        batch_tfm_cls = MultiChoiceTransform\n",
    "        before_batch_tfm = batch_tfm_cls(sentence_keys, ending_keys, pretrained_model_name=pretrained_model_name, \n",
    "                tokenizer_cls=tokenizer_cls, config=config, tokenizer=tokenizer, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL if group_by_len else TfmdDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=Undict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PadTokBatchTransform(Transform):\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, with_labels=False, padding=True, \n",
    "                 truncation=True, max_length=None, label_vocab=None,\n",
    "                 target_pad_id=-100, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        toks = [s[0] for s in samples]\n",
    "        # the labels are expected to be found either in dictionary with tokens\n",
    "        # or as element 1 of each sample\n",
    "        labels = ([s['labels'] for s in toks] \n",
    "                    if ('labels' in toks[0].keys()) else\n",
    "                    [s[1] for s in samples])\n",
    "\n",
    "        label_lens = [len(l) for l in labels]\n",
    "        max_label_length = max(label_lens)\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        for tok, label, label_len in zip(toks, labels, label_lens):\n",
    "            remainder = [self.target_pad_id] * (max_label_length - label_len)\n",
    "            tok[\"labels\"] = (label + remainder\n",
    "                             if padding_side==\"right\" else\n",
    "                             remainder + label)\n",
    "        inps = self.tokenizer.pad(toks,\n",
    "                              padding=self.padding,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        labels = inps.pop('labels')\n",
    "        inps = {k:TransTensorText(v) for k, v in inps.items() if (isinstance(v, torch.Tensor) and v.dim()>1)}\n",
    "        if self.with_labels:\n",
    "            inps['labels'] = labels\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            res = (inps, ) + (labels, )\n",
    "        return res\n",
    "\n",
    "    def decodes(self, x:TransTensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStrEx(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStrEx(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        return TitledStrEx(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))\n",
    "    def decodes(self, x):\n",
    "        if self.label_vocab is not None:\n",
    "            res = [self.label_vocab[e] for e in x if e != -100]\n",
    "        else:\n",
    "            res = [e for e in x if e != -100]\n",
    "        return TitledStrEx(''.join(f'{x}, ' for x in res), label='tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenClassificationBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for token classification using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(PadTokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, with_labels=True, label_vocab=None, \n",
    "                 group_by_len=True, **kwargs):\n",
    "        before_batch_tfm = PadTokBatchTransform(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls,\n",
    "                 config=config, tokenizer=tokenizer, label_vocab=label_vocab, with_labels=True, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL if group_by_len else TfmdDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=Undict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_learner.ipynb.\n",
      "Converted 02_metrics.ipynb.\n",
      "Converted 10_examples.classification-imdb.ipynb.\n",
      "Converted 11_examples.mlm-imdb.ipynb.\n",
      "Converted 12_examples.glue-benchmark.ipynb.\n",
      "Converted 12a_examples.glue-benchmark-sweeps.ipynb.\n",
      "Converted 14_examples.machine_translation.ipynb.\n",
      "Converted 15_examples.summarization.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

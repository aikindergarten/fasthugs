{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d953faa-7899-4e31-9532-844c98cad859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180c8ba-39ee-4669-a3e0-e7cda2a84268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9fe2f-9e29-4ee0-b58e-7508eb17a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ce3cc-e106-41f1-99f4-74a64d630d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ded12-1d97-44f7-ade4-b5c0ed8f3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from fastcore.all import *\n",
    "from fastai.basics import Transform, ItemTransform, Callback, ValueMetric\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "from datasets import load_metric\n",
    "try:\n",
    "    import nltk\n",
    "except: pass\n",
    "from fasthugs.learner import TransCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea26c9b-efcc-44d6-b294-1c3b86243470",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "> Utils for computing text metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe97eb9-e651-4f18-b6ed-8b2e2a007493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MetricCallback(Callback):\n",
    "    \"\"\"\n",
    "    Computes metric scores using HuggingFace datasets metric\n",
    "    \"\"\"\n",
    "    order = TransCallback.order + 2\n",
    "    run_train, run_valid = False, True\n",
    "    score_xtra_keys = {}\n",
    "\n",
    "    def before_fit(self): self.add_metrics()\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self.predict_with_generate:\n",
    "            preds = self.generated_tokens\n",
    "        else:\n",
    "            preds = self.pred.argmax(-1)\n",
    "        labels = self.yb[0] if len(self.yb) else self.trans.labels[0]\n",
    "\n",
    "        preproc_preds, preproc_labels = self.preprocess(preds, labels)\n",
    "        self.metric.add_batch(predictions=preproc_preds, references=preproc_labels)\n",
    "\n",
    "    def after_validate(self):\n",
    "        self.res = self.metric.compute()\n",
    "\n",
    "    def preprocess(self, predictions, labels):\n",
    "        return predictions, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_score(obj, score, **kwargs):\n",
    "        return obj.res[score]\n",
    "    \n",
    "    def _register_value_funcs(self):\n",
    "        for score in self.scores:\n",
    "            func = partial(self._get_score, self, score, **self.score_xtra_keys)\n",
    "            setattr(self, score, func)\n",
    "\n",
    "    def add_metrics(self):\n",
    "        learn_metrics = set([m.name for m in self.metrics])\n",
    "        for score_name in self.scores:\n",
    "            if not (score_name in learn_metrics):\n",
    "                self.learn.metrics += [ValueMetric(getattr(self, score_name), score_name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ce4d3-42ee-4859-9d93-180e27ac6fac",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba157775-62d5-4f15-b146-98929f94b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_score(obj, score_name, agg, measure):\n",
    "    return getattr(getattr(obj.res[score_name], agg), measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb095c-8bb5-4380-928c-448e5aec86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RougeScore(Callback):\n",
    "    \"\"\"\n",
    "    Computes ROUGE score using HF datasets metric. Adds `ValueMetric`\n",
    "    for each score in `scores`\n",
    "    \"\"\"\n",
    "    order = TransCallback.order + 2\n",
    "    run_train, run_valid = False, True\n",
    "    def __init__(self, tokenizer, argmax:bool=True, agg:str='mid', measure:str='fmeasure',\n",
    "                 scores:List[str]=['rouge1', 'rouge2', 'rougeL', 'rougeLsum']):\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        self.metric = load_metric(\"rouge\")\n",
    "        store_attr()\n",
    "        self._register_value_funcs()\n",
    "\n",
    "    def before_fit(self): self.add_metrics()\n",
    "    \n",
    "    def after_loss(self):\n",
    "        if self.predict_with_generate:\n",
    "            preds = self.generated_tokens\n",
    "        else:\n",
    "            preds = self.pred.argmax(-1)\n",
    "        labels = self.yb[0] if len(self.yb) else self.trans.labels[0]\n",
    "        # decode preds and labels\n",
    "        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = torch.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
    "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        # Rouge expects a newline after each sentence\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "        self.metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    def after_validate(self):\n",
    "        self.res = self.metric.compute()\n",
    "\n",
    "    def _register_value_funcs(self):\n",
    "        for score in self.scores:\n",
    "            func = partial(_get_score, self, score, self.agg, self.measure)\n",
    "            setattr(self, score, func)\n",
    "\n",
    "    def add_metrics(self):\n",
    "        learn_metrics = set([m.name for m in self.metrics])\n",
    "        for score_name in self.scores:\n",
    "            if not (score_name in learn_metrics):\n",
    "                self.learn.metrics += [ValueMetric(getattr(self, score_name), score_name)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be417959-d65c-45cc-a922-db97c912b3c2",
   "metadata": {},
   "source": [
    "## Seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e757472-1a84-4f93-9a6e-502039d1b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Seqeval(MetricCallback):\n",
    "    \"Computes scores using `seqeval` https://github.com/chakki-works/seqeval\"\n",
    "    def __init__(self, label_list, scores:Tuple[str]=('accuracy', 'f1', 'precision', 'recall')):\n",
    "        self.metric = load_metric('seqeval')\n",
    "        store_attr()\n",
    "        self._register_value_funcs()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_score(obj, score, **kwargs):\n",
    "        return obj.res[f\"overall_{score}\"]\n",
    "\n",
    "    def preprocess(self, predictions, labels):\n",
    "        \"Preprocess predictions and labels for seqeval, ref: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\"\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        return true_predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c5a6b-32e5-46df-a873-8d9f267a69e5",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca05a2c-1740-46a4-9d20-1f4e23ba9510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_learner.ipynb.\n",
      "Converted 02_metrics.ipynb.\n",
      "Converted 10_examples.classification-imdb.ipynb.\n",
      "Converted 11_examples.mlm-imdb.ipynb.\n",
      "Converted 12_examples.glue-benchmark.ipynb.\n",
      "Converted 12a_examples.glue-benchmark-sweeps.ipynb.\n",
      "Converted 14_examples.machine_translation.ipynb.\n",
      "Converted 15_examples.summarization.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dddd7-00d2-46f3-ad40-430430de81de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

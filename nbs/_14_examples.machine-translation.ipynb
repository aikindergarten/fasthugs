{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 28 11:42:45 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 960M    Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P8    N/A /  N/A |    312MiB /  4046MiB |     24%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1178      G   /usr/lib/xorg/Xorg                 37MiB |\n",
      "|    0   N/A  N/A      1864      G   /usr/lib/xorg/Xorg                175MiB |\n",
      "|    0   N/A  N/A      2041      G   /usr/bin/gnome-shell               33MiB |\n",
      "|    0   N/A  N/A     15009      G   /usr/lib/firefox/firefox            1MiB |\n",
      "|    0   N/A  N/A     15717      G   /usr/lib/firefox/firefox            1MiB |\n",
      "|    0   N/A  N/A     49957      G   ...AAAAAAAAA= --shared-files       47MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -Uqq fastai transformers datasets wandb\n",
    "    !pip install git+git://github.com/aikindergarten/fasthugs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification: IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, MarianConfig, AutoTokenizer, AutoConfig\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.wandb import *\n",
    "\n",
    "from fasthugs.learner import TransLearner\n",
    "from fasthugs.data import TransformersTextBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define main settings for the run in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name = 'imdb'\n",
    "model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "\n",
    "max_len = 512\n",
    "bs = 8\n",
    "val_bs = bs*2\n",
    "\n",
    "lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is light ?</td>\n",
       "      <td>Qu’est-ce que la lumière?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who are we?</td>\n",
       "      <td>Où sommes-nous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where did we come from?</td>\n",
       "      <td>D'où venons-nous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What would we do without it?</td>\n",
       "      <td>Que ferions-nous sans elle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?</td>\n",
       "      <td>Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             en  \\\n",
       "0                                                                               What is light ?   \n",
       "1                                                                                   Who are we?   \n",
       "2                                                                       Where did we come from?   \n",
       "3                                                                  What would we do without it?   \n",
       "4  What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?   \n",
       "\n",
       "                                                                                         fr  \n",
       "0                                                                 Qu’est-ce que la lumière?  \n",
       "1                                                                           Où sommes-nous?  \n",
       "2                                                                         D'où venons-nous?  \n",
       "3                                                              Que ferions-nous sans elle ?  \n",
       "4  Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/notebooks/data/questions_easy.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianConfig {\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"swish\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"MarianMTModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      59513\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_attention_heads\": 8,\n",
       "  \"decoder_ffn_dim\": 2048,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 59513,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 8,\n",
       "  \"encoder_ffn_dim\": 2048,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_length\": 512,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"marian\",\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 59513,\n",
       "  \"scale_embedding\": true,\n",
       "  \"static_position_embeddings\": true,\n",
       "  \"transformers_version\": \"4.6.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 59514\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokBatchTransform(Transform):\n",
    "    \"\"\"\n",
    "    Tokenizes texts in batches using pretrained HuggingFace tokenizer.\n",
    "    The first element in a batch can be single string or 2-tuple of strings.\n",
    "    If `with_labels=True` the \"labels\" are added to the output dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer, \n",
    "                 config=None, tokenizer=None, is_lm=False, with_labels=False,\n",
    "                 padding=True, truncation=True, max_length=None, \n",
    "                 do_targets=False, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kwargs = kwargs\n",
    "        self._two_texts = False\n",
    "        store_attr()\n",
    "    \n",
    "    def encodes(self, batch):\n",
    "        # batch is a list of tuples of ({text or (text1, text2)}, {targets...})\n",
    "        if is_listy(batch[0][0]): # 1st element is tuple\n",
    "            self._two_texts = True\n",
    "            texts = ([s[0][0] for s in batch], [s[0][1] for s in batch])\n",
    "        elif is_listy(batch[0]): \n",
    "            texts = ([s[0] for s in batch],)\n",
    "        else: # batch is list of texts\n",
    "            texts = (list(batch),)\n",
    "            batch = [(s, ) for s in batch]\n",
    "        inps = self.tokenizer(*texts,\n",
    "                              add_special_tokens=True,\n",
    "                              padding=self.padding,\n",
    "                              truncation=self.truncation,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors='pt',\n",
    "                              **self.kwargs)\n",
    "        \n",
    "        if self.do_targets and isinstance(batch[0][1], str):\n",
    "            target_texts = [s[1] for s in batch]\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                targets = self.tokenizer(target_texts,\n",
    "                                  padding=self.padding,\n",
    "                                  truncation=self.truncation,\n",
    "                                  max_length=self.max_length,\n",
    "                                  return_tensors='pt', \n",
    "                                  **self.kwargs).input_ids\n",
    "            inps['labels'] = targets\n",
    "            res = (inps, )\n",
    "        else:\n",
    "            # inps are batched, collate targets into batches too\n",
    "            labels = default_collate([s[1:] for s in batch])\n",
    "            if self.with_labels:\n",
    "                # TODO consider cases when there are multiple labels\n",
    "                inps['labels'] = labels[0]\n",
    "                res = (inps, )\n",
    "            else:\n",
    "                res = (inps, ) + tuple(labels)\n",
    "        return res\n",
    "    \n",
    "    def decodes(self, x:TensorText):\n",
    "        if self._two_texts:\n",
    "            x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n",
    "            return (TitledStr(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n",
    "                    TitledStr(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersTextBlock(TransformBlock):\n",
    "    \"A `TransformBlock` for texts using pretrained tokenizers from Huggingface\"\n",
    "    @delegates(TokBatchTransform)\n",
    "    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n",
    "                 config=None, tokenizer=None, preprocessed=False, **kwargs):\n",
    "        batch_tfm_cls = PadBatchTransform if preprocessed else TokBatchTransform\n",
    "        before_batch_tfm = batch_tfm_cls(pretrained_model_name=pretrained_model_name, tokenizer_cls=tokenizer_cls,\n",
    "                 config=config, tokenizer=tokenizer, **kwargs)\n",
    "        return super().__init__(dl_type=SortedDL,\n",
    "                                dls_kwargs={'before_batch': before_batch_tfm,\n",
    "                                            'create_batch': fa_convert},\n",
    "                                batch_tfms=Undict()\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasthugs.data import TextGetter, Undict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.transform import ItemTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ItemTransform\n",
    "def untuple1(x):\n",
    "    return (*x[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(\n",
    "    blocks = [TransformersTextBlock(tokenizer=tokenizer, do_targets=True)],\n",
    "    get_x=TextGetter('fr', 'en'),\n",
    "    item_tfms=untuple1,\n",
    "    splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from                                                                                                  en  \\\n",
      "0                                                                                   What is light ?   \n",
      "1                                                                                       Who are we?   \n",
      "2                                                                           Where did we come from?   \n",
      "3                                                                      What would we do without it?   \n",
      "4      What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?   \n",
      "...                                                                                             ...   \n",
      "40133                                                         What actions can lead to termination?   \n",
      "40134                                           What makes one crew do a go-around and another not?   \n",
      "40135                                                    What is this situation elsewhere in world?   \n",
      "40136                                 What rationale lies behind the claim for pre-emptive strikes?   \n",
      "40137                       Why is it not only an American issue but also an international reality?   \n",
      "\n",
      "                                                                                             fr  \n",
      "0                                                                     Qu’est-ce que la lumière?  \n",
      "1                                                                               Où sommes-nous?  \n",
      "2                                                                             D'où venons-nous?  \n",
      "3                                                                  Que ferions-nous sans elle ?  \n",
      "4      Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?  \n",
      "...                                                                                         ...  \n",
      "40133                                 Quels sont les gestes qui peuvent mener au congédiement ?  \n",
      "40134           Qu’est-ce qui motive un équipage à exécuter une remise des gaz et pas un autre?  \n",
      "40135      Comment cette situation se compare-t-elle à celle en vigueur ailleurs dans le monde?  \n",
      "40136                                     Quelle est la justification des attaques préemptives?  \n",
      "40137                    Pourquoi cette question touche-t-elle non seulement les États-Unis mai  \n",
      "\n",
      "[40138 rows x 2 columns]\n",
      "Found 40138 items\n",
      "2 datasets of sizes 32111,8027\n",
      "Setting up Pipeline: TextGetter\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: TextGetter\n",
      "    starting from\n",
      "      en              What about the negotiation process itself and the role of potential partners?\n",
      "fr    Qu’en est-il du processus de négociation lui-même et du rôle des partenaires éventuels?\n",
      "Name: 8059, dtype: object\n",
      "    applying TextGetter gives\n",
      "      (Qu’en est-il du processus de négociation lui-même et du rôle des partenaires éventuels?, What about the negotiation process itself and the role of potential partners?)\n",
      "\n",
      "Final sample: (('Qu’en est-il du processus de négociation lui-même et du rôle des partenaires éventuels?', 'What about the negotiation process itself and the role of potential partners?'),)\n",
      "\n",
      "\n",
      "Collecting items from                                                                                                  en  \\\n",
      "0                                                                                   What is light ?   \n",
      "1                                                                                       Who are we?   \n",
      "2                                                                           Where did we come from?   \n",
      "3                                                                      What would we do without it?   \n",
      "4      What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?   \n",
      "...                                                                                             ...   \n",
      "40133                                                         What actions can lead to termination?   \n",
      "40134                                           What makes one crew do a go-around and another not?   \n",
      "40135                                                    What is this situation elsewhere in world?   \n",
      "40136                                 What rationale lies behind the claim for pre-emptive strikes?   \n",
      "40137                       Why is it not only an American issue but also an international reality?   \n",
      "\n",
      "                                                                                             fr  \n",
      "0                                                                     Qu’est-ce que la lumière?  \n",
      "1                                                                               Où sommes-nous?  \n",
      "2                                                                             D'où venons-nous?  \n",
      "3                                                                  Que ferions-nous sans elle ?  \n",
      "4      Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?  \n",
      "...                                                                                         ...  \n",
      "40133                                 Quels sont les gestes qui peuvent mener au congédiement ?  \n",
      "40134           Qu’est-ce qui motive un équipage à exécuter une remise des gaz et pas un autre?  \n",
      "40135      Comment cette situation se compare-t-elle à celle en vigueur ailleurs dans le monde?  \n",
      "40136                                     Quelle est la justification des attaques préemptives?  \n",
      "40137                    Pourquoi cette question touche-t-elle non seulement les États-Unis mai  \n",
      "\n",
      "[40138 rows x 2 columns]\n",
      "Found 40138 items\n",
      "2 datasets of sizes 32111,8027\n",
      "Setting up Pipeline: TextGetter\n",
      "Setting up after_item: Pipeline: untuple1 -> ToTensor\n",
      "Setting up before_batch: Pipeline: TokBatchTransform\n",
      "Setting up after_batch: Pipeline: Undict\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: untuple1 -> ToTensor\n",
      "    starting from\n",
      "      ((Qu’en est-il du processus de négociation lui-même et du rôle des partenaires éventuels?, What about the negotiation process itself and the role of potential partners?))\n",
      "    applying untuple1 gives\n",
      "      (Qu’en est-il du processus de négociation lui-même et du rôle des partenaires éventuels?, What about the negotiation process itself and the role of potential partners?)\n",
      "    applying ToTensor gives\n",
      "      (Qu’en est-il du processus de négociation lui-même et du rôle des partenaires éventuels?, What about the negotiation process itself and the role of potential partners?)\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "Applying before_batch to the list of samples\n",
      "  Pipeline: TokBatchTransform\n",
      "    starting from\n",
      "      [(Qu’en est-il du processus de négociation lui-même et du rôle des partenaires éventuels?, What about the negotiation process itself and the role of potential partners?), (Pourquoi les responsables des SSNA apportent­ils ces changements ?, Why is NIHB making this change?), (Qui sont les étudiants de l’IIG?, Who are the students attending IIG?), (Pourquoi la route est-elle souvent longue entre les déclarations d’intention et leur mise en œuvre effective ?, Why is it often a long stretch from declarations of intent to actual implementation?)]\n",
      "    applying TokBatchTransform gives\n",
      "      ({'input_ids': tensor([[ 1276,     1,   162,    43,    21,   107,    22,   572,     5,  6395,\n",
      "           316,    21,  2197,    11,    22,   843,    13,  1712, 14601,    54,\n",
      "             0, 59513],\n",
      "        [ 1955,    16,  2641,    13, 16695,  4779, 17820,     1,   489,   129,\n",
      "          1871,    99,     0, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513],\n",
      "        [ 2361,    70,    16,  3540,     5,    14,     1,  5327,   612,    54,\n",
      "             0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513],\n",
      "        [ 1955,     8,  2020,    43,    21,   431,  1320,  3549,   164,    16,\n",
      "          3632,    20,     1,  3966,    11,   125,   355,    23,   755,   873,\n",
      "            99,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  430,   200,     4, 12896,   509,  2241,    10,     4,   908,     7,\n",
      "          1589,  1986,    54,     0, 59513, 59513],\n",
      "        [ 1932,    32, 48001,  1529,    67,   713,    54,     0, 59513, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513],\n",
      "        [ 3271,    52,     4,  2376, 15664,    47,  6389,    54,     0, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513],\n",
      "        [ 1932,    32,    61,  1526,    15,   409, 23393,    64, 16139,     7,\n",
      "         15157,    12,  4297,   554,    54,     0]])})\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: Undict\n",
      "    starting from\n",
      "      ({'input_ids': tensor([[ 1276,     1,   162,    43,    21,   107,    22,   572,     5,  6395,\n",
      "           316,    21,  2197,    11,    22,   843,    13,  1712, 14601,    54,\n",
      "             0, 59513],\n",
      "        [ 1955,    16,  2641,    13, 16695,  4779, 17820,     1,   489,   129,\n",
      "          1871,    99,     0, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513],\n",
      "        [ 2361,    70,    16,  3540,     5,    14,     1,  5327,   612,    54,\n",
      "             0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513],\n",
      "        [ 1955,     8,  2020,    43,    21,   431,  1320,  3549,   164,    16,\n",
      "          3632,    20,     1,  3966,    11,   125,   355,    23,   755,   873,\n",
      "            99,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'labels': tensor([[  430,   200,     4, 12896,   509,  2241,    10,     4,   908,     7,\n",
      "          1589,  1986,    54,     0, 59513, 59513],\n",
      "        [ 1932,    32, 48001,  1529,    67,   713,    54,     0, 59513, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513],\n",
      "        [ 3271,    52,     4,  2376, 15664,    47,  6389,    54,     0, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513],\n",
      "        [ 1932,    32,    61,  1526,    15,   409, 23393,    64, 16139,     7,\n",
      "         15157,    12,  4297,   554,    54,     0]], device='cuda:0')})\n",
      "    applying Undict gives\n",
      "      ({'input_ids': tensor([[ 1276,     1,   162,    43,    21,   107,    22,   572,     5,  6395,\n",
      "           316,    21,  2197,    11,    22,   843,    13,  1712, 14601,    54,\n",
      "             0, 59513],\n",
      "        [ 1955,    16,  2641,    13, 16695,  4779, 17820,     1,   489,   129,\n",
      "          1871,    99,     0, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513],\n",
      "        [ 2361,    70,    16,  3540,     5,    14,     1,  5327,   612,    54,\n",
      "             0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513],\n",
      "        [ 1955,     8,  2020,    43,    21,   431,  1320,  3549,   164,    16,\n",
      "          3632,    20,     1,  3966,    11,   125,   355,    23,   755,   873,\n",
      "            99,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'labels': tensor([[  430,   200,     4, 12896,   509,  2241,    10,     4,   908,     7,\n",
      "          1589,  1986,    54,     0, 59513, 59513],\n",
      "        [ 1932,    32, 48001,  1529,    67,   713,    54,     0, 59513, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513],\n",
      "        [ 3271,    52,     4,  2376, 15664,    47,  6389,    54,     0, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513],\n",
      "        [ 1932,    32,    61,  1526,    15,   409, 23393,    64, 16139,     7,\n",
      "         15157,    12,  4297,   554,    54,     0]], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "dblock.summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 4.18 ms, total: 14.7 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bs = 16\n",
    "dls = dblock.dataloaders(df, bs=bs, val_bs=bs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 3625, 12775,  1538,  ..., 19226,    54,     0],\n",
       "          [  350,    19,   898,  ..., 59513, 59513, 59513],\n",
       "          [ 1590,    15,    21,  ..., 59513, 59513, 59513],\n",
       "          ...,\n",
       "          [ 4717,    43,     8,  ..., 59513, 59513, 59513],\n",
       "          [ 1276,     6,    82,  ..., 59513, 59513, 59513],\n",
       "          [ 1955,    16, 31548,  ..., 59513, 59513, 59513]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'labels': tensor([[  430, 11156,  1538,  ...,   632,    54,     0],\n",
       "          [ 9074,     4,   898,  ..., 59513, 59513, 59513],\n",
       "          [  430,   373,    45,  ..., 59513, 59513, 59513],\n",
       "          ...,\n",
       "          [  430,    32,     4,  ..., 59513, 59513, 59513],\n",
       "          [  430,   664,    84,  ..., 59513, 59513, 59513],\n",
       "          [ 1932,    52,  1509,  ..., 59513, 59513, 59513]], device='cuda:0')},)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9154b19e5b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, b, max_n, ctxs, show, unique, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_show_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_show_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_get_idxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/fastcore/dispatch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/fasthugs/fasthugs/data.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(x, y, samples, ctxs, max_n, trunc_at, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrunc_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mctxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mdisplay_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(x, y, samples, ctxs, max_n, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mctxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mctxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mctxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mctxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "dls.show_batch(max_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking with W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes some details on w&b tracking and the leaderboard to be established..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "WANDB_NAME = f'{ds_name}-{model_name}'\n",
    "GROUP = f'{ds_name}-{model_name}-simple-{lr:.0e}'\n",
    "NOTES = f'finetuning {model_name} with RAdam lr={lr:.0e}'\n",
    "CONFIG = {}\n",
    "TAGS =[model_name, ds_name, 'radam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfastai_community\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.28 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">imdb-distilroberta-base</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fastai_community/fasthugs\" target=\"_blank\">https://wandb.ai/fastai_community/fasthugs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/fastai_community/fasthugs/runs/18cwvdx0\" target=\"_blank\">https://wandb.ai/fastai_community/fasthugs/runs/18cwvdx0</a><br/>\n",
       "                Run data is saved locally in <code>/notebooks/fasthugs/nbs/wandb/run-20210429_183326-18cwvdx0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_output\n",
    "wandb.init(reinit=True, project=\"fasthugs\", entity=\"fastai_community\",\n",
    "           name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS, config=CONFIG);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "learn = TransLearner(dls, model, metrics=CorpusBLEUMetric(), loss_func=noop).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>corpus_bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.208636</td>\n",
       "      <td>8.356507</td>\n",
       "      <td>0.056414</td>\n",
       "      <td>06:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.910537</td>\n",
       "      <td>8.097847</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>06:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianModel(\n",
       "  (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "  (encoder): MarianEncoder(\n",
       "    (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "    (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0): MarianEncoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): MarianEncoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): MarianEncoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): MarianEncoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): MarianEncoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): MarianEncoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): MarianDecoder(\n",
       "    (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "    (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0): MarianDecoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): MarianDecoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): MarianDecoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): MarianDecoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): MarianDecoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): MarianDecoderLayer(\n",
       "        (self_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MarianAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-9.0407e-03,  2.8072e-02,  1.4343e-02,  ..., -3.6860e-02,\n",
       "         -4.2824e-02,  9.6411e-03],\n",
       "        [ 3.2397e-03,  3.7629e-02,  1.3613e-02,  ..., -1.7274e-02,\n",
       "         -4.1874e-03, -3.3502e-02],\n",
       "        [ 3.9743e-03,  2.9589e-02,  1.3218e-02,  ..., -3.3958e-02,\n",
       "          2.0799e-02, -1.6180e-02],\n",
       "        ...,\n",
       "        [ 2.5598e-02,  1.8182e-02,  1.2885e-02,  ...,  1.1126e-02,\n",
       "          2.1903e-02,  3.5934e-03],\n",
       "        [-9.4439e-03, -6.3931e-03,  2.4074e-03,  ..., -1.0516e-02,\n",
       "          1.6802e-02, -2.4351e-02],\n",
       "        [ 1.0829e-09,  1.3621e-08, -1.2080e-08,  ..., -1.1688e-08,\n",
       "          6.7046e-09, -4.8634e-09]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-9.0407e-03,  2.8072e-02,  1.4343e-02,  ..., -3.6860e-02,\n",
       "         -4.2824e-02,  9.6411e-03],\n",
       "        [ 3.2397e-03,  3.7629e-02,  1.3613e-02,  ..., -1.7274e-02,\n",
       "         -4.1874e-03, -3.3502e-02],\n",
       "        [ 3.9743e-03,  2.9589e-02,  1.3218e-02,  ..., -3.3958e-02,\n",
       "          2.0799e-02, -1.6180e-02],\n",
       "        ...,\n",
       "        [ 2.5598e-02,  1.8182e-02,  1.2885e-02,  ...,  1.1126e-02,\n",
       "          2.1903e-02,  3.5934e-03],\n",
       "        [-9.4439e-03, -6.3931e-03,  2.4074e-03,  ..., -1.0516e-02,\n",
       "          1.6802e-02, -2.4351e-02],\n",
       "        [ 1.0829e-09,  1.3621e-08, -1.2080e-08,  ..., -1.1688e-08,\n",
       "          6.7046e-09, -4.8634e-09]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.shared.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-9.0407e-03,  2.8072e-02,  1.4343e-02,  ..., -3.6860e-02,\n",
       "         -4.2824e-02,  9.6411e-03],\n",
       "        [ 3.2397e-03,  3.7629e-02,  1.3613e-02,  ..., -1.7274e-02,\n",
       "         -4.1874e-03, -3.3502e-02],\n",
       "        [ 3.9743e-03,  2.9589e-02,  1.3218e-02,  ..., -3.3958e-02,\n",
       "          2.0799e-02, -1.6180e-02],\n",
       "        ...,\n",
       "        [ 2.5598e-02,  1.8182e-02,  1.2885e-02,  ...,  1.1126e-02,\n",
       "          2.1903e-02,  3.5934e-03],\n",
       "        [-9.4439e-03, -6.3931e-03,  2.4074e-03,  ..., -1.0516e-02,\n",
       "          1.6802e-02, -2.4351e-02],\n",
       "        [ 1.0829e-09,  1.3621e-08, -1.2080e-08,  ..., -1.1688e-08,\n",
       "          6.7046e-09, -4.8634e-09]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.encoder.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.base_model.decoder.embed_tokens.weight == model.base_model.encoder.embed_tokens.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.lm_head.weight == model.base_model.shared.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There's a sign on The Lost Highway that says:&lt;br /&gt;&lt;br /&gt;*MAJOR SPOILERS AHEAD*&lt;br /&gt;&lt;br /&gt;(but you already knew that, didn't you?)&lt;br /&gt;&lt;br /&gt;Since there's a great deal of people that apparently did not get the point of this movie, I'd like to contribute my interpretation of why the plot makes perfect sense. As others have pointed out, one single viewing of this movie is not sufficient. If you have the DVD of MD, you can \"cheat\" by looking at David Lynch's \"Top 10 Hints to Unlocking MD\" (but only upon second or third viewing, please.) ;)&lt;br /&gt;&lt;br /&gt;First of all, Mulholland Drive is downright brilliant. A masterpiece. This is the kind of movie that refuse to leave your head. Not often are the comments on the DVDs very accurate, but Vogue's \"It gets inside your head and stays there\" really hit the mark.&lt;br /&gt;&lt;br /&gt;David Lynch deserves praise for creating</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, what did I expect? I thought this would be a film about young adults at their turning-point in life, something like \"Sonnenallee\" or \"American Pie\", which I liked a lot. I wanted to see a funny film, perhaps with an ironic look on idyllic Wuerzburg. And what did I get?&lt;br /&gt;&lt;br /&gt;Attention, spoilers ahead!&lt;br /&gt;&lt;br /&gt;This film starts with a lengthy dialogue which gives you a good hint of what will inevitably follow: more lengthy dialogues. Sometimes I thought Moritz Bleibtreu might have forgotten his text and trying to hide that fact by improvising and just repeating what he was saying before. But as I think of Bleibtreu as one of the better german actors, I believe that this effect really was intended. I think the author wanted to show how boring talking to close friends can be - especially when they are stoned. But really, I don't need</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This was one of the most dishonest, meaningless, and non-peaceful of the films I have ever seen. The representation of the other, of the Israelis, was racist, backward, and unfair. For one, the song played on E.S' car radio when pulled up alongside a very right-wing Israeli driver was \"I put a spell on you\" by Natacha Atlas. The song's style is quite Arabic, but it was released on an Israeli compilation CD, and I have even heard it on the radio in Israel. Many Israeli songs (as well as architecture, foods, and slang) are influenced by Arabic culture, and there is no reason an Israeli Jew would be offended or angered by a nearby car playing that song. The way E.S. appears so calm and collected with his sunglasses and cool glare, via a long, still shot, is meant to force the viewer into seeing the Jew as haggard</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pistol-packing Pam Grier takes names and kicks butt as the heroine in \"Asylum of Satan\" director William Girdler's entertaining blaxploitation actioneer \"Sheba Baby,\" co-starring D'Urville Martin and Austin Stoker. \"Sheba Baby\" is one of several tough chick flicks that Grier appeared in during the 1970s, including \"Coffy,\" \"Foxy Brown,\" and \"Friday Foster.\" The short-lived Girdler co-wrote this thoroughly routine private eye potboiler with producer David Shelton in one night and it features a headstrong female shamus that refuses to rely on a man to help her take care of business. Unfortunately, \"Sheba Baby\" isn't nearly as good as the blaxploitation movies that Grier made under the supervision of director Jack Hill. Hill helmed the African-American North Carolina native in \"Coffy,\" \"Foxy Brown,\" \"The Big Bird Cage,\" and \"The Big Doll House.\" Anybody that analyzes images of African-American women in cinema should be familiar with these epics. The chief problem with</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once again I must play something of the contrarian. Most of the reviews for Ab Tak Chappan have been extremely positive. Mine is positive, but only slightly. A 7 out of 10 is equivalent to a \"C\" letter grade from me.&lt;br /&gt;&lt;br /&gt;It seems that a lot of the praise is rooted in two factors: One, that Ab Tak Chappan is more realistic than the typical Bollywood film, and two, that it is trying to do things differently.&lt;br /&gt;&lt;br /&gt;The first point I couldn't care less about. I'm not looking for realism in films, and so I do not score higher for a film that shows a story and characters closer to how I believe the real world to be--I'm a big fan of surrealism, fantasy, absurdism, and so on, although I do not dislike realist films merely for the fact that they're realist.&lt;br /&gt;&lt;br /&gt;For the second point, I</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Master of Italian horror Dario Argento is called a lot of bad things by non-fans. And is deserving of absolutely none of the backlash. In fact, every time I hear something bad about Argento- I think they're really talking about Michele Soavi. He just doesn't get the same amount of attention because his films were never as successful in theaters. In fact, his best film - 1994's Cemetery Man - was probably his least successful. Or just didn't get the attention he felt it deserved, because after that, he left film and went into directing television. He's never gone back. So people really don't know how inferior his other films are because by the time they've seen them, they're already fans of the Italian horror aesthetic. Which means you have to accept the fact that they make almost zero sense and are usually very unattractive films. This is where The</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wallace and Gromit are the main characters in some of the best cartoons ever crafted. The excellent mix of visual humor and claymation makes \"A Grand Day Out,\" \"The Wrong Trousers,\" and also \"A Close Shave\" some of the best animated footage ever put on television. Winning several Oscars and also countless other awards, Nick Park became quite the popular man in the U.K., yet his impact on the United States has not been big. After the third Wallace and Gromit short, there was all this speculation about a full-length Wallace and Gromit movie, yet for years nothing had happened. Then in 2000 instead of a full-length Wallace and Gromit film, we get another brilliant claymation film from Nick Park, which was Chicken Run, which almost got nominated for best picture in the Academy Awards. Perhaps it was the success of this film that ultimately drove Park to finally work</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It's almost impossible for me to sit down and write a conscientious review of THREE COLORS: RED without letting people in on some of the ideas that Krysztoff Kieszlowski has explored in the previous two entries to this fascinating trilogy. The more I see them and think of them, and imagine myself in their world, the more I get its theme: that we are more linked to each other than we would want to think ourselves, and all it takes is a little hand of fate to set some events in motion. In BLUE, Juliette Binoche played a grieving widow whose plan to live her life without connections to the past had her meet someone unexpected. In WHITE, an act of cruelty spawns an unlikely friendship between two men who will, against the odds, conspire to bring the perpetrator to justice and full circle. And now, in RED, all the</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A blaxploitation classic, this movie was terribly influential in rap music for the \"toasts\" that Rudy Ray Moore performs. Toasts are long rhyming stories that are funny and deliver a point, and you can see how they would naturally evolve into rap. For more on toasts, Rudy Ray Moore, and why this movie is important, go to Dolemite.com.&lt;br /&gt;&lt;br /&gt;Which leaves us just to talk about the movie itself. This movie packs in a great deal of \"laugh-at-the-funny-outfits-and-hairstyles\" bang for the buck, as nearly every shot has some sort of outrageous element or dialogue. It starts as Dolemite is being released from prison in order to find out who framed him and bring him to justice. I was unaware that prisons release people so they can prove their own innocence, but that's me, I'm a neophyte in the prison scene. He is helped in this by Queen Bee, who is</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

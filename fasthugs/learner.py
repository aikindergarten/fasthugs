# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_learner.ipynb (unless otherwise specified).

__all__ = ['default_splitter', 'to_device', 'TransCallback', 'TransLearner']

# Cell
from fastai.basics import *
from fastai.text.all import TensorText
from inspect import signature
from .data import TransformersTextBlock

from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, BatchEncoding
from transformers.modeling_outputs import QuestionAnsweringModelOutput

# Cell
def default_splitter(model):
    groups = L(model.base_model.children()) + L(m for m in list(model.children())[1:] if params(m))
    return groups.map(params)

# Cell
@typedispatch
def show_results(x: TensorText, y, samples, outs, ctxs=None, max_n=10, trunc_at=150, **kwargs):
    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))
    if isinstance(samples[0][0], tuple):
        samples = L((*s[0], *s[1:]) for s in samples)
        if trunc_at is not None: samples = L((s[0].truncate(trunc_at), s[1].truncate(trunc_at), *s[2:]) for s in samples)
    elif trunc_at is not None: samples = L((s[0].truncate(trunc_at),*s[1:]) for s in samples)
    ctxs = show_results[object](x, y, samples, outs, ctxs=ctxs, max_n=max_n, **kwargs)
    display_df(pd.DataFrame(ctxs))
    return ctxs

# Cell
def to_device(b, device=None):
    "Recursively put `b` on `device`. Handles `BatchEncoding`s"
    if defaults.use_cuda==False: device='cpu'
    elif device is None: device=default_device()
    def _inner(o):
        if isinstance(o,Tensor): return o.to(device, non_blocking=True)
        elif isinstance(o, (dict, BatchEncoding)):
            return {k:to_device(v) for k,v in o.items()}
        elif hasattr(o, "to_device"): return o.to_device(device)
        else: return o
    return apply(_inner, b)

# Cell
class TransCallback(Callback):
    "Handles usecase with loss returned by HuggingFace model"
    def after_pred(self):
        if 'loss' in self.pred:
            self.learn.loss_grad = self.pred.loss
            self.learn.loss = self.pred.loss.clone()
            if 'labels' in self.xb[0].keys():
                self.learn.yb = (self.xb[0]['labels'], )
            self.learn.compute_loss = False
        if isinstance(self.pred, QuestionAnsweringModelOutput):
            self.learn.pred = (self.pred.start_logits, self.pred.end_logits)
        else: self.learn.pred = self.pred.logits

# Cell
@delegates(Learner.__init__)
class TransLearner(Learner):
    "Learner for training transformers from HuggingFace"
    def __init__(self, dls, model, **kwargs):
        splitter = kwargs.get('splitter', None)
        if splitter is None: kwargs['splitter'] = default_splitter
        super().__init__(dls, model, **kwargs)
        self.model_args = set(signature(model.forward).parameters.keys())
        self.add_cb(TransCallback())
        self.compute_loss = True

    def one_batch(self, i, b):
        self.iter = i
        b_on_device = tuple(to_device(e) for e in b) if self.dls.device is not None else b
        self._split(b_on_device)
        self._with_events(self._do_one_batch, 'batch', CancelBatchException)

    def _do_one_batch(self):
        x = self.xb[0]
        for k in x.keys():
            if k not in self.model_args: del x[k]
        self.pred = self.model(**self.x)
        self('after_pred')
        if len(self.yb) and self.compute_loss:
            self.loss_grad = self.loss_func(self.pred, *self.yb)
            self.loss = self.loss_grad.clone()
        self('after_loss')
        if not self.training or not len(self.yb): return
        self('before_backward')
        self.loss_grad.backward()
        self._with_events(self.opt.step, 'step', CancelStepException)
        self.opt.zero_grad()